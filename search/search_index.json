{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TorchFlare is a simple, beginner-friendly and an easy-to-use PyTorch Framework train your models without much effort. It provides an almost Keras-like experience for training your models with all the callbacks, metrics, etc Features A high-level module for Keras-like training. Off-the-shelf Dataloaders for standard tasks(Classification, Regression, etc) Callbacks for model checkpoints, early stopping, and much more! Metrics and much more. Currently, TorchFlare supports CPU and GPU training. DDP and TPU support will be coming soon! Getting Started The core idea around TorchFlare is the Experiment class. It handles all the internal stuff like boiler plate code for training, calling callbacks,metrics,etc. The only thing you need to focus on is creating you PyTorch Model. Also, there are off-the-shelf dataloaders available for standard tasks, so that you don't have to worry about creating Pytorch Datasets. Here is an easy-to-understand example to show how Experiment class works. import torch import torch.nn as nn from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics #Some dummy dataloaders train_dl = SomeTrainingDataloader () valid_dl = SomeValidationDataloader () test_dl = SomeTestingDataloader () Create a pytorch Model model = nn . Sequential ( nn . Linear ( num_features , hidden_state_size ), nn . ReLU (), nn . Linear ( hidden_state_size , num_classes ) ) Define callbacks and metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" )] Define your experiment # Set some constants for training exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"model.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) # Compile your experiment with model, optimizer, schedulers, etc exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 5 ), criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) # Run your experiment with training dataloader and validation dataloader. # Both Training and validation dataloaders are required for training. exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) For inference you can use infer method, which yields output per batch. You can use it as follows outputs = [] for op in exp . infer ( test_loader = test_dl , path = './models/model.bin' , device = 'cuda' ): op = some_post_process_function ( op ) outputs . extend ( op ) Experiment class internally saves a history.csv file which includes your training and validation metrics per epoch. This file can be found in same directory as save_dir argument. If you want to access your experiments history or plot it. You can do it as follows. history = exp . history . history # This will return a dict # If you want to plot progress of particular metric as epoch progress use this. exp . plot_history ( key = \"accuracy\" , save_fig = False , plot_fig = True )","title":"TorchFlare"},{"location":"#features","text":"A high-level module for Keras-like training. Off-the-shelf Dataloaders for standard tasks(Classification, Regression, etc) Callbacks for model checkpoints, early stopping, and much more! Metrics and much more. Currently, TorchFlare supports CPU and GPU training. DDP and TPU support will be coming soon!","title":"Features"},{"location":"#getting-started","text":"The core idea around TorchFlare is the Experiment class. It handles all the internal stuff like boiler plate code for training, calling callbacks,metrics,etc. The only thing you need to focus on is creating you PyTorch Model. Also, there are off-the-shelf dataloaders available for standard tasks, so that you don't have to worry about creating Pytorch Datasets. Here is an easy-to-understand example to show how Experiment class works. import torch import torch.nn as nn from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics #Some dummy dataloaders train_dl = SomeTrainingDataloader () valid_dl = SomeValidationDataloader () test_dl = SomeTestingDataloader () Create a pytorch Model model = nn . Sequential ( nn . Linear ( num_features , hidden_state_size ), nn . ReLU (), nn . Linear ( hidden_state_size , num_classes ) ) Define callbacks and metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" )] Define your experiment # Set some constants for training exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"model.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) # Compile your experiment with model, optimizer, schedulers, etc exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 5 ), criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) # Run your experiment with training dataloader and validation dataloader. # Both Training and validation dataloaders are required for training. exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) For inference you can use infer method, which yields output per batch. You can use it as follows outputs = [] for op in exp . infer ( test_loader = test_dl , path = './models/model.bin' , device = 'cuda' ): op = some_post_process_function ( op ) outputs . extend ( op ) Experiment class internally saves a history.csv file which includes your training and validation metrics per epoch. This file can be found in same directory as save_dir argument. If you want to access your experiments history or plot it. You can do it as follows. history = exp . history . history # This will return a dict # If you want to plot progress of particular metric as epoch progress use this. exp . plot_history ( key = \"accuracy\" , save_fig = False , plot_fig = True )","title":"Getting Started"},{"location":"dataloader/","text":"Class to create easy to use dataloaders. Methods image_data_from_df ( path , df , image_col , label_cols = None , augmentations = None , convert_mode = 'RGB' , extension = None ) classmethod Classmethod to create a dataset for image data when you have image names/ids , labels in dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None label_cols Union[str, List[str]] The list of columns containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description SimpleDataloader Pytorch dataset created from dataframe. For inference do not pass in the label_cols , keep it None . Augmentations must be Compose objects from albumentations or torchvision . image_data_from_folders ( path , augmentations = None , convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description SimpleDataloader Pytorch image dataset created from folders Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg segmentation_data_from_folders ( image_path , mask_path = None , augmentations = None , image_convert_mode = 'L' , mask_convert_mode = 'L' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SimpleDataloader Pytorch Segmentation dataset created from folders. tabular_data_from_csv ( csv_path , feature_cols , label_cols = None ) classmethod Classmethod to create a dataset for tabular data from csv. Parameters: Name Type Description Default csv_path str The full path to csv. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description SimpleDataloader Tabular pytorch dataset. tabular_data_from_df ( df , feature_cols , label_cols = None ) classmethod Classmethod to create dataset for tabular data from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe containing features and labels. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description SimpleDataloader Tabular pytorch dataset Examples image_data_from_df from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . image_data_from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augs , extension = './jpg' ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) image_data_from_folders from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . image_data_from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) segmentation_data_from_rle from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . segmentation_data_from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) segmentation_data_from_folders from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . segmentation_data_from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) tabular_data_from_df from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . tabular_data_from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) tabular_data_from_csv from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . tabular_data_from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.) text_classification_data_from_df from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . text_classification_data_from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args. )","title":"SimpleDataloader"},{"location":"dataloader/#torchflare.datasets.dataloaders.SimpleDataloader-methods","text":"","title":"Methods"},{"location":"dataloader/#torchflare.datasets.dataloaders.SimpleDataloader.image_data_from_df","text":"Classmethod to create a dataset for image data when you have image names/ids , labels in dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None label_cols Union[str, List[str]] The list of columns containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description SimpleDataloader Pytorch dataset created from dataframe. For inference do not pass in the label_cols , keep it None . Augmentations must be Compose objects from albumentations or torchvision .","title":"image_data_from_df()"},{"location":"dataloader/#torchflare.datasets.dataloaders.SimpleDataloader.image_data_from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description SimpleDataloader Pytorch image dataset created from folders Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg","title":"image_data_from_folders()"},{"location":"dataloader/#torchflare.datasets.dataloaders.SimpleDataloader.segmentation_data_from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Optional[Union[A.Compose, torchvision.transforms.Compose]] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SimpleDataloader Pytorch Segmentation dataset created from folders.","title":"segmentation_data_from_folders()"},{"location":"dataloader/#torchflare.datasets.dataloaders.SimpleDataloader.tabular_data_from_csv","text":"Classmethod to create a dataset for tabular data from csv. Parameters: Name Type Description Default csv_path str The full path to csv. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description SimpleDataloader Tabular pytorch dataset.","title":"tabular_data_from_csv()"},{"location":"dataloader/#torchflare.datasets.dataloaders.SimpleDataloader.tabular_data_from_df","text":"Classmethod to create dataset for tabular data from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe containing features and labels. required feature_cols Union[str, List[str]] name(str) or list containing names feature columns. required label_cols Union[str, List[str]] name(str) or list containing names label columns. None Returns: Type Description SimpleDataloader Tabular pytorch dataset","title":"tabular_data_from_df()"},{"location":"dataloader/#examples","text":"","title":"Examples"},{"location":"dataloader/#image_data_from_df","text":"from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . image_data_from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augs , extension = './jpg' ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"image_data_from_df"},{"location":"dataloader/#image_data_from_folders","text":"from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . image_data_from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"image_data_from_folders"},{"location":"dataloader/#segmentation_data_from_rle","text":"from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . segmentation_data_from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"segmentation_data_from_rle"},{"location":"dataloader/#segmentation_data_from_folders","text":"from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . segmentation_data_from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"segmentation_data_from_folders"},{"location":"dataloader/#tabular_data_from_df","text":"from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . tabular_data_from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"tabular_data_from_df"},{"location":"dataloader/#tabular_data_from_csv","text":"from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . tabular_data_from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args.)","title":"tabular_data_from_csv"},{"location":"dataloader/#text_classification_data_from_df","text":"from torchflare.datasets import SimpleDataloader dl = SimpleDataloader . text_classification_data_from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 64 , # Required Args. shuffle = True , # Required Args. num_workers = 0 , # keyword Args. collate_fn = collate_fn # keyword Args. )","title":"text_classification_data_from_df"},{"location":"experiment/","text":"Simple Experiment for handling boilerplate code for training, validation and Inference. Methods __init__ ( self , num_epochs , save_dir = './exp_outputs' , model_name = 'model.bin' , fp16 = False , device = 'cuda' , compute_train_metrics = False , using_batch_mixers = False , seed = 42 ) special Init method to set up important variables for training and validation. Parameters: Name Type Description Default num_epochs int The number of epochs to save model. required save_dir str The directory where to save the model, and the log files. './exp_outputs' model_name str The name of '.bin' file. Defaults to 'model.bin' 'model.bin' fp16 bool Set this to True if you want to use mixed precision training(Default : False) False device str The device where you want train your model. 'cuda' compute_train_metrics bool Whether to compute metrics on training data as well False using_batch_mixers bool Whether using special batch_mixers like cutmix or mixup. False seed int The seed to ensure reproducibility. 42 Note If batch_mixers are used then set compute_train_metrics to False. Also, only validation metrics will be computed if batch_mixers are used. compile_experiment ( self , model , optimizer , optimizer_params , criterion , scheduler = None , scheduler_params = None , callbacks = None , metrics = None , main_metric = None , resume_checkpoint = False ) Configures the model for training and validation. Parameters: Name Type Description Default model Module The model to be trained. required optimizer Union[torch.optim.optimizer.Optimizer, str, Any] The optimizer to be used or name of optimizer. If you pass in the name of the optimizer, only optimizers available in pytorch are supported. required optimizer_params Dict[str, Union[int, float]] The parameters to be used for the optimizer. required scheduler Optional[str] The scheduler or the name of the scheduler. If you pass in the name of the scheduler, only scheduler available in pytorch/transformers can be supported. None scheduler_params Dict[str, Union[int, float, str]] The parameters to be used for the scheduler. None criterion Union[Callable[[torch.Tensor], torch.Tensor], str] The loss function to optimize or name of the loss function. If you pass in the name of the loss function, only loss functions available in pytorch can be supported. required callbacks List The list of callbacks to be used. None metrics List The list of metrics to be used. None main_metric Optional[str] The name of main metric to be monitored. Use lower case version. For examples , use 'accuracy' instead of 'Accuracy'. None resume_checkpoint bool Whether to resume training from the saved model. False Note Supports all the schedulers implemented in pytorch/transformers except SWA. Support for custom scheduling will be added soon. infer ( self , test_loader , path , device = 'cuda' ) Method to perform inference on test dataloader. Parameters: Name Type Description Default test_loader DataLoader The dataloader to be use for testing. required device str The device on which you want to perform inference. 'cuda' path str The path where the '.bin' or '.pt' or '.pth' file is saved. example: path = \"/output/model.bin\" required Yields Output per batch perform_sanity_check ( self , dl ) Method to check if the model forward pass and loss_computation is working or not. Parameters: Name Type Description Default dl DataLoader A PyTorch dataloader. required run_experiment ( self , train_dl , valid_dl ) Method to train and validate the model for fixed number of epochs. Parameters: Name Type Description Default train_dl DataLoader The training dataloader. required valid_dl DataLoader The validation dataloader. required Note Model will only be saved when ModelCheckpoint callback is used. Examples import torch import torchflare.callbacks as cbs import torchflare.metrics as metrics from torchflare.experiments import Experiment # Defining Training/Validation Dataloaders train_dl = SomeTrainDataloader () valid_dl = SomeValidDataloader () # Defining some basic model model = SomeModel () # Defining params optimizer = \"Adam\" optimizer_params = dict ( lr = 1e-4 ) scheduler = \"ReduceLROnPlateau\" scheduler_params = dict ( mode = \"max\" ) criterion = \"cross_entropy\" num_epochs = 10 num_classes = 4 # Defining the list of metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False ), ] # Defining the list of callbacks callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" ), ] # Creating Experiment and setting the params. exp = Experiment ( num_epochs = num_epochs , save_dir = \"./test_save\" , model_name = \"test_classification.bin\" , fp16 = True , device = device , seed = 42 , using_batch_mixers = False , compute_train_metrics = False , ) # Compiling the experiment exp . compile_experiment ( model = model , metrics = metric_list , callbacks = callbacks , main_metric = \"accuracy\" , optimizer = optimizer , optimizer_params = optimizer_params , scheduler = scheduler , scheduler_params = scheduler_params , criterion = criterion , ) # Performing sanity check(optional) exp . perform_sanity_check ( train_dl ) # Running the experiment exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl )","title":"Experiment"},{"location":"experiment/#torchflare.experiments.experiment.Experiment-methods","text":"","title":"Methods"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.__init__","text":"Init method to set up important variables for training and validation. Parameters: Name Type Description Default num_epochs int The number of epochs to save model. required save_dir str The directory where to save the model, and the log files. './exp_outputs' model_name str The name of '.bin' file. Defaults to 'model.bin' 'model.bin' fp16 bool Set this to True if you want to use mixed precision training(Default : False) False device str The device where you want train your model. 'cuda' compute_train_metrics bool Whether to compute metrics on training data as well False using_batch_mixers bool Whether using special batch_mixers like cutmix or mixup. False seed int The seed to ensure reproducibility. 42 Note If batch_mixers are used then set compute_train_metrics to False. Also, only validation metrics will be computed if batch_mixers are used.","title":"__init__()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.compile_experiment","text":"Configures the model for training and validation. Parameters: Name Type Description Default model Module The model to be trained. required optimizer Union[torch.optim.optimizer.Optimizer, str, Any] The optimizer to be used or name of optimizer. If you pass in the name of the optimizer, only optimizers available in pytorch are supported. required optimizer_params Dict[str, Union[int, float]] The parameters to be used for the optimizer. required scheduler Optional[str] The scheduler or the name of the scheduler. If you pass in the name of the scheduler, only scheduler available in pytorch/transformers can be supported. None scheduler_params Dict[str, Union[int, float, str]] The parameters to be used for the scheduler. None criterion Union[Callable[[torch.Tensor], torch.Tensor], str] The loss function to optimize or name of the loss function. If you pass in the name of the loss function, only loss functions available in pytorch can be supported. required callbacks List The list of callbacks to be used. None metrics List The list of metrics to be used. None main_metric Optional[str] The name of main metric to be monitored. Use lower case version. For examples , use 'accuracy' instead of 'Accuracy'. None resume_checkpoint bool Whether to resume training from the saved model. False Note Supports all the schedulers implemented in pytorch/transformers except SWA. Support for custom scheduling will be added soon.","title":"compile_experiment()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.infer","text":"Method to perform inference on test dataloader. Parameters: Name Type Description Default test_loader DataLoader The dataloader to be use for testing. required device str The device on which you want to perform inference. 'cuda' path str The path where the '.bin' or '.pt' or '.pth' file is saved. example: path = \"/output/model.bin\" required Yields Output per batch","title":"infer()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.perform_sanity_check","text":"Method to check if the model forward pass and loss_computation is working or not. Parameters: Name Type Description Default dl DataLoader A PyTorch dataloader. required","title":"perform_sanity_check()"},{"location":"experiment/#torchflare.experiments.experiment.Experiment.run_experiment","text":"Method to train and validate the model for fixed number of epochs. Parameters: Name Type Description Default train_dl DataLoader The training dataloader. required valid_dl DataLoader The validation dataloader. required Note Model will only be saved when ModelCheckpoint callback is used.","title":"run_experiment()"},{"location":"experiment/#examples","text":"import torch import torchflare.callbacks as cbs import torchflare.metrics as metrics from torchflare.experiments import Experiment # Defining Training/Validation Dataloaders train_dl = SomeTrainDataloader () valid_dl = SomeValidDataloader () # Defining some basic model model = SomeModel () # Defining params optimizer = \"Adam\" optimizer_params = dict ( lr = 1e-4 ) scheduler = \"ReduceLROnPlateau\" scheduler_params = dict ( mode = \"max\" ) criterion = \"cross_entropy\" num_epochs = 10 num_classes = 4 # Defining the list of metrics metric_list = [ metrics . Accuracy ( num_classes = num_classes , multilabel = False ), metrics . F1Score ( num_classes = num_classes , multilabel = False ), ] # Defining the list of callbacks callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , mode = \"max\" ), cbs . ModelCheckpoint ( monitor = \"accuracy\" ), ] # Creating Experiment and setting the params. exp = Experiment ( num_epochs = num_epochs , save_dir = \"./test_save\" , model_name = \"test_classification.bin\" , fp16 = True , device = device , seed = 42 , using_batch_mixers = False , compute_train_metrics = False , ) # Compiling the experiment exp . compile_experiment ( model = model , metrics = metric_list , callbacks = callbacks , main_metric = \"accuracy\" , optimizer = optimizer , optimizer_params = optimizer_params , scheduler = scheduler , scheduler_params = scheduler_params , criterion = criterion , ) # Performing sanity check(optional) exp . perform_sanity_check ( train_dl ) # Running the experiment exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl )","title":"Examples"},{"location":"Tutorials/hydra/","text":"Image Classification with torchflare and Hydra This tutorial will guide on how to do image classification using torchflare. We will also use hydra:cc to manage our parameters. Dataset: https://www.kaggle.com/c/cifar-10 Importing Libraries from hydra.experimental import compose , initialize from omegaconf import OmegaConf from hydra.utils import * import numpy as np import pandas as pd import albumentations as A from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms from torchflare.datasets import SimpleDataloader , show_batch from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics Loading and Preparing the dataset df = pd . read_csv ( \"trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True ) # Shuffling the dataframe test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 ) # Splitting into train and validation data. Defining a Simple Model. class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () Defining basic transforms. train_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) valid_transform = transforms . Compose ([ transforms . ToTensor ()]) Creating Dataloaders We will be using SimpleDataloaders from torchflare to easily create the dataloaders. # Creating Training Dataloader. train_dl = SimpleDataloader . image_data_from_df ( df = train_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) # Creating Validation Dataloader. valid_dl = SimpleDataloader . image_data_from_df ( df = valid_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = False , num_workers = 0 ) Defining Callbacks and metrics. We will be using callbacks and metrics defined in torchflare library. callbacks = [ instantiate ( cfg . callbacks . early_stopping ), instantiate ( cfg . callbacks . model_checkpoint )] metrics = [ instantiate ( cfg . metric )] Setting up the Experiment exp = Experiment ( ** cfg . experiment . constant_params ) exp . compile_experiment ( model = Net (), callbacks = callbacks , metrics = metrics , ** cfg . experiment . compile_params ) exp . perform_sanity_check ( train_dl ) exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) Running Inference data = dict ( cfg . shared_data_params ) # popping label_cols, since for test we dont need those. _ = data . pop ( \"label_cols\" ) test_dl = SimpleDataloader . image_data_from_df ( df = test_df , augmentations = test_transform , ** data ) . get_loader ( batch_size = 32 , shuffle = False ) # Inference ops = [] for op in exp . infer ( path = \"./models/cifar10.bin\" , test_loader = test_dl ): _ , y_pred = torch . max ( op , dim = 1 ) ops . extend ( y_pred ) Visualizing History plot_metrics = [ \"loss\" , \"accuracy\" ] for metric in plot_metrics : exp . plot_history ( key = metric , save_fig = False , plot_fig = True ) Notebook is available in examples folder.","title":"hydra"},{"location":"Tutorials/hydra/#image-classification-with-torchflare-and-hydra","text":"This tutorial will guide on how to do image classification using torchflare. We will also use hydra:cc to manage our parameters. Dataset: https://www.kaggle.com/c/cifar-10","title":"Image Classification with torchflare and Hydra"},{"location":"Tutorials/hydra/#importing-libraries","text":"from hydra.experimental import compose , initialize from omegaconf import OmegaConf from hydra.utils import * import numpy as np import pandas as pd import albumentations as A from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms from torchflare.datasets import SimpleDataloader , show_batch from torchflare.experiments import Experiment import torchflare.callbacks as cbs import torchflare.metrics as metrics","title":"Importing Libraries"},{"location":"Tutorials/hydra/#loading-and-preparing-the-dataset","text":"df = pd . read_csv ( \"trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True ) # Shuffling the dataframe test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 ) # Splitting into train and validation data.","title":"Loading and Preparing the dataset"},{"location":"Tutorials/hydra/#defining-a-simple-model","text":"class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net ()","title":"Defining a Simple Model."},{"location":"Tutorials/hydra/#defining-basic-transforms","text":"train_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) valid_transform = transforms . Compose ([ transforms . ToTensor ()])","title":"Defining basic transforms."},{"location":"Tutorials/hydra/#creating-dataloaders","text":"We will be using SimpleDataloaders from torchflare to easily create the dataloaders. # Creating Training Dataloader. train_dl = SimpleDataloader . image_data_from_df ( df = train_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) # Creating Validation Dataloader. valid_dl = SimpleDataloader . image_data_from_df ( df = valid_df , augmentations = transform , ** cfg . shared_data_params ) . get_loader ( batch_size = 32 , shuffle = False , num_workers = 0 )","title":"Creating Dataloaders"},{"location":"Tutorials/hydra/#defining-callbacks-and-metrics","text":"We will be using callbacks and metrics defined in torchflare library. callbacks = [ instantiate ( cfg . callbacks . early_stopping ), instantiate ( cfg . callbacks . model_checkpoint )] metrics = [ instantiate ( cfg . metric )]","title":"Defining Callbacks and metrics."},{"location":"Tutorials/hydra/#setting-up-the-experiment","text":"exp = Experiment ( ** cfg . experiment . constant_params ) exp . compile_experiment ( model = Net (), callbacks = callbacks , metrics = metrics , ** cfg . experiment . compile_params ) exp . perform_sanity_check ( train_dl ) exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl )","title":"Setting up the Experiment"},{"location":"Tutorials/hydra/#running-inference","text":"data = dict ( cfg . shared_data_params ) # popping label_cols, since for test we dont need those. _ = data . pop ( \"label_cols\" ) test_dl = SimpleDataloader . image_data_from_df ( df = test_df , augmentations = test_transform , ** data ) . get_loader ( batch_size = 32 , shuffle = False ) # Inference ops = [] for op in exp . infer ( path = \"./models/cifar10.bin\" , test_loader = test_dl ): _ , y_pred = torch . max ( op , dim = 1 ) ops . extend ( y_pred )","title":"Running Inference"},{"location":"Tutorials/hydra/#visualizing-history","text":"plot_metrics = [ \"loss\" , \"accuracy\" ] for metric in plot_metrics : exp . plot_history ( key = metric , save_fig = False , plot_fig = True ) Notebook is available in examples folder.","title":"Visualizing History"},{"location":"Tutorials/image_classification/","text":"Image Classification with torchflare Hey there, this tutorial will guide on how to do image classification using torchflare. Dataset: https://www.kaggle.com/c/cifar-10 Importing Libraries import numpy as np import pandas as pd import albumentations as A from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms from torchflare.datasets import SimpleDataloader , show_batch from torchflare.experiments import Experiment import torchflare.callbacks as cbs from torchflare.metrics import Accuracy Loading and Preparing the dataset df = pd . read_csv ( \"trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True ) # Shuffling the dataframe test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 ) # Splitting into train and validation data. Defining a Simple Model. class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () Defining basic transforms. train_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) valid_transform = transforms . Compose ([ transforms . ToTensor ()]) Creating Dataloaders We will be using SimpleDataloaders from torchflare to easily create the dataloaders. # Creating Training Dataloader. train_dl = SimpleDataloader . image_data_from_df ( path = \"./train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = train_transform , df = train_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) # Creating Validation Dataloader. valid_dl = SimpleDataloader . image_data_from_df ( path = \"./train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = valid_transform , df = valid_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 16 , num_workers = 0 ) # Creating Test Dataloader. test_dl = SimpleDataloader . image_data_from_df ( path = \"./train\" , image_col = \"id\" , label_cols = None , # Setting label_cols as None since we wont have labels for test data. augmentations = valid_transform , df = test_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 16 , num_workers = 0 ) Visualizing a batch from train dataloader. show_batch ( train_dl ) Defining Callbacks and metrics. We will be using callbacks and metrics defined in torchflare library. metric_list = [ Accuracy ( num_classes = len ( classes ), multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = acc . handle (), patience = 5 ), cbs . ModelCheckpoint ( monitor = acc . handle ()), ] Setting up the experiment and Training our model. 1. The first step is to setup some constants and params for the experiment. exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"cifar10.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) 2. We will compile our experiment where we will define our optimizer, scheduler,etc exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 5 ), criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) 3. This step is optional, but it's good to perform a check to see if the defined model's forward pass is working or not and check if loss computation is working. exp . perform_sanity_check ( train_dl ) 4. Now we run our experiment with our training and validation dataloaders. We use fastprogress as our progress bar hence the output will be like fast.ai, a nice table with our metrics, loss and time. exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) 5. The infer method yields output of every batch so that you can perform any kind of post-processing on your outputs. ops = [] # Since infer method yeilds we use a for loop. for op in exp . infer ( path = \"./models/cifar10.bin\" , test_loader = test_dl ): op = post_process_func ( op ) ops . extend ( y_pred ) 6. Want to visualize model history ? # I want to visualize train_accuracy/valid_accuracy against epochs. exp . plot_history ( key = \"accuracy\" , save_fig = False , plot_fig = True ) Notebook is available in examples folder.","title":"Image Data"},{"location":"Tutorials/image_classification/#image-classification-with-torchflare","text":"Hey there, this tutorial will guide on how to do image classification using torchflare. Dataset: https://www.kaggle.com/c/cifar-10","title":"Image Classification with torchflare"},{"location":"Tutorials/image_classification/#importing-libraries","text":"import numpy as np import pandas as pd import albumentations as A from sklearn.model_selection import train_test_split import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms from torchflare.datasets import SimpleDataloader , show_batch from torchflare.experiments import Experiment import torchflare.callbacks as cbs from torchflare.metrics import Accuracy","title":"Importing Libraries"},{"location":"Tutorials/image_classification/#loading-and-preparing-the-dataset","text":"df = pd . read_csv ( \"trainLabels.csv\" ) classes = df . label . unique () . tolist () class_to_idx = { value : key for key , value in enumerate ( classes )} df . label = df . label . map ( class_to_idx ) df . id = df . id . astype ( str ) df = df . sample ( frac = 1 ) . reset_index ( drop = True ) # Shuffling the dataframe test_df = df . iloc [: 10000 , :] # I took first 10000 entries as test data data_df = df . iloc [ 10000 :, :] train_df , valid_df = train_test_split ( data_df , test_size = 0.3 ) # Splitting into train and validation data.","title":"Loading and Preparing the dataset"},{"location":"Tutorials/image_classification/#defining-a-simple-model","text":"class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net ()","title":"Defining a Simple Model."},{"location":"Tutorials/image_classification/#defining-basic-transforms","text":"train_transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))] ) valid_transform = transforms . Compose ([ transforms . ToTensor ()])","title":"Defining basic transforms."},{"location":"Tutorials/image_classification/#creating-dataloaders","text":"We will be using SimpleDataloaders from torchflare to easily create the dataloaders. # Creating Training Dataloader. train_dl = SimpleDataloader . image_data_from_df ( path = \"./train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = train_transform , df = train_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 32 , shuffle = True , num_workers = 0 ) # Creating Validation Dataloader. valid_dl = SimpleDataloader . image_data_from_df ( path = \"./train\" , image_col = \"id\" , label_cols = \"label\" , augmentations = valid_transform , df = valid_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 16 , num_workers = 0 ) # Creating Test Dataloader. test_dl = SimpleDataloader . image_data_from_df ( path = \"./train\" , image_col = \"id\" , label_cols = None , # Setting label_cols as None since we wont have labels for test data. augmentations = valid_transform , df = test_df , extension = \".png\" , convert_mode = \"RGB\" , ) . get_loader ( batch_size = 16 , num_workers = 0 ) Visualizing a batch from train dataloader. show_batch ( train_dl )","title":"Creating Dataloaders"},{"location":"Tutorials/image_classification/#defining-callbacks-and-metrics","text":"We will be using callbacks and metrics defined in torchflare library. metric_list = [ Accuracy ( num_classes = len ( classes ), multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = acc . handle (), patience = 5 ), cbs . ModelCheckpoint ( monitor = acc . handle ()), ]","title":"Defining Callbacks and metrics."},{"location":"Tutorials/image_classification/#setting-up-the-experiment-and-training-our-model","text":"1. The first step is to setup some constants and params for the experiment. exp = Experiment ( num_epochs = 5 , save_dir = \"./models\" , model_name = \"cifar10.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) 2. We will compile our experiment where we will define our optimizer, scheduler,etc exp . compile_experiment ( model = net , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 5 ), criterion = \"cross_entropy\" , metrics = metric_list , main_metric = \"accuracy\" , ) 3. This step is optional, but it's good to perform a check to see if the defined model's forward pass is working or not and check if loss computation is working. exp . perform_sanity_check ( train_dl ) 4. Now we run our experiment with our training and validation dataloaders. We use fastprogress as our progress bar hence the output will be like fast.ai, a nice table with our metrics, loss and time. exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) 5. The infer method yields output of every batch so that you can perform any kind of post-processing on your outputs. ops = [] # Since infer method yeilds we use a for loop. for op in exp . infer ( path = \"./models/cifar10.bin\" , test_loader = test_dl ): op = post_process_func ( op ) ops . extend ( y_pred ) 6. Want to visualize model history ? # I want to visualize train_accuracy/valid_accuracy against epochs. exp . plot_history ( key = \"accuracy\" , save_fig = False , plot_fig = True ) Notebook is available in examples folder.","title":"Setting up the experiment and Training our model."},{"location":"Tutorials/tabular_data/","text":"Classification on Tabular data with torchflare Let's learn how to perform classification for tasks involving tabular data. Dataset: https://www.kaggle.com/c/cat-in-the-dat-ii/overview Importing Libraries import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader import os import copy import traceback import datetime import random from torchflare.experiments import Experiment import torchflare.metrics as metrics import torchflare.callbacks as cbs import torchflare.criterion as crit from torchflare.datasets import SimpleDataloader Some Utility Functions. def reduce_mem_usage ( df , verbose = True ): ''' Reduce file memory usage Source: https://www.kaggle.com/artgor Parameters: ----------- df: DataFrame Dataset on which to perform transformation verbose: bool Print additional information Returns: -------- DataFrame Dataset as pandas DataFrame ''' numerics = [ 'int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64' ] start_mem = df . memory_usage () . sum () / 1024 ** 2 for col in df . columns : col_type = df [ col ] . dtypes if col_type in numerics : c_min = df [ col ] . min () c_max = df [ col ] . max () if str ( col_type )[: 3 ] == 'int' : if c_min > np . iinfo ( np . int8 ) . min and c_max < np . iinfo ( np . int8 ) . max : df [ col ] = df [ col ] . astype ( np . int8 ) elif c_min > np . iinfo ( np . int16 ) . min and c_max < np . iinfo ( np . int16 ) . max : df [ col ] = df [ col ] . astype ( np . int16 ) elif c_min > np . iinfo ( np . int32 ) . min and c_max < np . iinfo ( np . int32 ) . max : df [ col ] = df [ col ] . astype ( np . int32 ) elif c_min > np . iinfo ( np . int64 ) . min and c_max < np . iinfo ( np . int64 ) . max : df [ col ] = df [ col ] . astype ( np . int64 ) else : c_prec = df [ col ] . apply ( lambda x : np . finfo ( x ) . precision ) . max () if c_min > np . finfo ( np . float16 ) . min and c_max < np . finfo ( np . float16 ) \\ . max and c_prec == np . finfo ( np . float16 ) . precision : df [ col ] = df [ col ] . astype ( np . float16 ) elif c_min > np . finfo ( np . float32 ) . min and c_max < np . finfo ( np . float32 ) \\ . max and c_prec == np . finfo ( np . float32 ) . precision : df [ col ] = df [ col ] . astype ( np . float32 ) else : df [ col ] = df [ col ] . astype ( np . float64 ) end_mem = df . memory_usage () . sum () / 1024 ** 2 if verbose : print ( 'Mem. usage decreased to {:5.2f} Mb ( {:.1f}% r eduction)' \\ . format ( end_mem , 100 * ( start_mem - end_mem ) / start_mem )) return ( df ) Reading and preprocessing the data def perform_label_enc ( feature_col , df ): label_encoders = {} for cat_col in feature_col : label_encoders [ cat_col ] = LabelEncoder () df [ cat_col ] = label_encoders [ cat_col ] . fit_transform ( df [ cat_col ] . astype ( \"category\" ) . cat . codes . fillna ( - 1 ) . values ) return df train_df = pd . read_csv ( \"./dataset/train.csv\" ) features = train_df . columns . difference ([ \"id\" , \"target\" ]) . tolist () target = \"target\" train_df = perform_label_enc ( feature_col = features , df = train_df ) train_df = reduce_mem_usage ( train_df ) cat_dims = [ int ( train_df [ col ] . nunique ()) for col in features ] emb_dims = [( x , min ( 50 , ( x + 1 ) // 2 )) for x in cat_dims ] train_df , valid_df = train_test_split ( train_df , test_size = 0.3 , stratify = train_df . target ) Defining the Model class Model ( nn . Module ): def __init__ ( self , emb_dims , lin_layer_sizes , output_size , emb_dropout , lin_layer_dropouts ): \"\"\" Model uses entity embeddings. Paper: https://arxiv.org/abs/1604.06737 Args: emb_dims: List of two element tuples For each categorical feature the first element of a tuple will denote the number of unique values of the categorical feature. The second element will denote the embedding dimension to be used for that feature. \"\"\" super ( Model , self ) . __init__ () # Embedding layers self . emb_layers = nn . ModuleList ([ nn . Embedding ( x , y ) for x , y in emb_dims ]) self . no_of_embs = sum ([ y for x , y in emb_dims ]) # Linear Layers first_lin_layer = nn . Linear ( in_features = self . no_of_embs , out_features = lin_layer_sizes [ 0 ] ) self . lin_layers = nn . ModuleList ( [ first_lin_layer ] + [ nn . Linear ( lin_layer_sizes [ i ], lin_layer_sizes [ i + 1 ]) for i in range ( len ( lin_layer_sizes ) - 1 ) ] ) for lin_layer in self . lin_layers : nn . init . kaiming_normal_ ( lin_layer . weight . data ) # Output Layer self . output_layer = nn . Linear ( lin_layer_sizes [ - 1 ], output_size ) nn . init . kaiming_normal_ ( self . output_layer . weight . data ) # Batch Norm Layers self . first_bn_layer = nn . BatchNorm1d ( self . no_of_embs ) self . bn_layers = nn . ModuleList ( [ nn . BatchNorm1d ( size ) for size in lin_layer_sizes ] ) # Dropout Layers self . emb_dropout_layer = nn . Dropout ( emb_dropout ) self . droput_layers = nn . ModuleList ( [ nn . Dropout ( size ) for size in lin_layer_dropouts ] ) def forward ( self , cat_data ): if self . no_of_embs != 0 : x = [ emb_layer ( cat_data [:, i ]) for i , emb_layer in enumerate ( self . emb_layers ) ] x = torch . cat ( x , 1 ) x = self . first_bn_layer ( x ) x = self . emb_dropout_layer ( x ) for lin_layer , dropout_layer , bn_layer in zip ( self . lin_layers , self . droput_layers , self . bn_layers ): x = F . relu ( lin_layer ( x )) x = dropout_layer ( x ) x = bn_layer ( x ) x = self . output_layer ( x ) return x model = Model ( emb_dims , lin_layer_sizes = [ 300 , 300 ], output_size = 1 , emb_dropout = 0.3 , lin_layer_dropouts = [ 0.3 , 0.3 ]) Creating the dataloaders train_dl = SimpleDataloader . tabular_data_from_df ( df = train_df , feature_cols = features , label_cols = target ) . get_loader ( batch_size = 32 , num_workers = 0 , shuffle = True ) valid_dl = SimpleDataloader . tabular_data_from_df ( df = valid_df , feature_cols = features , label_cols = target ) . get_loader ( batch_size = 32 , num_workers = 0 , shuffle = False ) Defining metrics and callbacks. metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False , threshold = 0.6 )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , patience = 5 ), cbs . ModelCheckpoint ( monitor = \"accuracy\" ), ] Setting up the Experiment exp = Experiment ( num_epochs = 10 , save_dir = \"./models\" , model_name = \"tabular_cls.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) exp . compile_experiment ( model = model , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 2 ), criterion = crit . BCEWithLogitsFlat , metrics = metric_list , main_metric = \"accuracy\" , ) Run the experiment. exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) Here is a snapshot of how progress bar looks(Same as fastai, since we are using fastprogess). Notebook is available in examples folder.","title":"Tabular Data"},{"location":"Tutorials/tabular_data/#classification-on-tabular-data-with-torchflare","text":"Let's learn how to perform classification for tasks involving tabular data. Dataset: https://www.kaggle.com/c/cat-in-the-dat-ii/overview","title":"Classification on Tabular data with torchflare"},{"location":"Tutorials/tabular_data/#importing-libraries","text":"import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader import os import copy import traceback import datetime import random from torchflare.experiments import Experiment import torchflare.metrics as metrics import torchflare.callbacks as cbs import torchflare.criterion as crit from torchflare.datasets import SimpleDataloader","title":"Importing Libraries"},{"location":"Tutorials/tabular_data/#some-utility-functions","text":"def reduce_mem_usage ( df , verbose = True ): ''' Reduce file memory usage Source: https://www.kaggle.com/artgor Parameters: ----------- df: DataFrame Dataset on which to perform transformation verbose: bool Print additional information Returns: -------- DataFrame Dataset as pandas DataFrame ''' numerics = [ 'int16' , 'int32' , 'int64' , 'float16' , 'float32' , 'float64' ] start_mem = df . memory_usage () . sum () / 1024 ** 2 for col in df . columns : col_type = df [ col ] . dtypes if col_type in numerics : c_min = df [ col ] . min () c_max = df [ col ] . max () if str ( col_type )[: 3 ] == 'int' : if c_min > np . iinfo ( np . int8 ) . min and c_max < np . iinfo ( np . int8 ) . max : df [ col ] = df [ col ] . astype ( np . int8 ) elif c_min > np . iinfo ( np . int16 ) . min and c_max < np . iinfo ( np . int16 ) . max : df [ col ] = df [ col ] . astype ( np . int16 ) elif c_min > np . iinfo ( np . int32 ) . min and c_max < np . iinfo ( np . int32 ) . max : df [ col ] = df [ col ] . astype ( np . int32 ) elif c_min > np . iinfo ( np . int64 ) . min and c_max < np . iinfo ( np . int64 ) . max : df [ col ] = df [ col ] . astype ( np . int64 ) else : c_prec = df [ col ] . apply ( lambda x : np . finfo ( x ) . precision ) . max () if c_min > np . finfo ( np . float16 ) . min and c_max < np . finfo ( np . float16 ) \\ . max and c_prec == np . finfo ( np . float16 ) . precision : df [ col ] = df [ col ] . astype ( np . float16 ) elif c_min > np . finfo ( np . float32 ) . min and c_max < np . finfo ( np . float32 ) \\ . max and c_prec == np . finfo ( np . float32 ) . precision : df [ col ] = df [ col ] . astype ( np . float32 ) else : df [ col ] = df [ col ] . astype ( np . float64 ) end_mem = df . memory_usage () . sum () / 1024 ** 2 if verbose : print ( 'Mem. usage decreased to {:5.2f} Mb ( {:.1f}% r eduction)' \\ . format ( end_mem , 100 * ( start_mem - end_mem ) / start_mem )) return ( df )","title":"Some Utility Functions."},{"location":"Tutorials/tabular_data/#reading-and-preprocessing-the-data","text":"def perform_label_enc ( feature_col , df ): label_encoders = {} for cat_col in feature_col : label_encoders [ cat_col ] = LabelEncoder () df [ cat_col ] = label_encoders [ cat_col ] . fit_transform ( df [ cat_col ] . astype ( \"category\" ) . cat . codes . fillna ( - 1 ) . values ) return df train_df = pd . read_csv ( \"./dataset/train.csv\" ) features = train_df . columns . difference ([ \"id\" , \"target\" ]) . tolist () target = \"target\" train_df = perform_label_enc ( feature_col = features , df = train_df ) train_df = reduce_mem_usage ( train_df ) cat_dims = [ int ( train_df [ col ] . nunique ()) for col in features ] emb_dims = [( x , min ( 50 , ( x + 1 ) // 2 )) for x in cat_dims ] train_df , valid_df = train_test_split ( train_df , test_size = 0.3 , stratify = train_df . target )","title":"Reading and preprocessing the data"},{"location":"Tutorials/tabular_data/#defining-the-model","text":"class Model ( nn . Module ): def __init__ ( self , emb_dims , lin_layer_sizes , output_size , emb_dropout , lin_layer_dropouts ): \"\"\" Model uses entity embeddings. Paper: https://arxiv.org/abs/1604.06737 Args: emb_dims: List of two element tuples For each categorical feature the first element of a tuple will denote the number of unique values of the categorical feature. The second element will denote the embedding dimension to be used for that feature. \"\"\" super ( Model , self ) . __init__ () # Embedding layers self . emb_layers = nn . ModuleList ([ nn . Embedding ( x , y ) for x , y in emb_dims ]) self . no_of_embs = sum ([ y for x , y in emb_dims ]) # Linear Layers first_lin_layer = nn . Linear ( in_features = self . no_of_embs , out_features = lin_layer_sizes [ 0 ] ) self . lin_layers = nn . ModuleList ( [ first_lin_layer ] + [ nn . Linear ( lin_layer_sizes [ i ], lin_layer_sizes [ i + 1 ]) for i in range ( len ( lin_layer_sizes ) - 1 ) ] ) for lin_layer in self . lin_layers : nn . init . kaiming_normal_ ( lin_layer . weight . data ) # Output Layer self . output_layer = nn . Linear ( lin_layer_sizes [ - 1 ], output_size ) nn . init . kaiming_normal_ ( self . output_layer . weight . data ) # Batch Norm Layers self . first_bn_layer = nn . BatchNorm1d ( self . no_of_embs ) self . bn_layers = nn . ModuleList ( [ nn . BatchNorm1d ( size ) for size in lin_layer_sizes ] ) # Dropout Layers self . emb_dropout_layer = nn . Dropout ( emb_dropout ) self . droput_layers = nn . ModuleList ( [ nn . Dropout ( size ) for size in lin_layer_dropouts ] ) def forward ( self , cat_data ): if self . no_of_embs != 0 : x = [ emb_layer ( cat_data [:, i ]) for i , emb_layer in enumerate ( self . emb_layers ) ] x = torch . cat ( x , 1 ) x = self . first_bn_layer ( x ) x = self . emb_dropout_layer ( x ) for lin_layer , dropout_layer , bn_layer in zip ( self . lin_layers , self . droput_layers , self . bn_layers ): x = F . relu ( lin_layer ( x )) x = dropout_layer ( x ) x = bn_layer ( x ) x = self . output_layer ( x ) return x model = Model ( emb_dims , lin_layer_sizes = [ 300 , 300 ], output_size = 1 , emb_dropout = 0.3 , lin_layer_dropouts = [ 0.3 , 0.3 ])","title":"Defining the Model"},{"location":"Tutorials/tabular_data/#creating-the-dataloaders","text":"train_dl = SimpleDataloader . tabular_data_from_df ( df = train_df , feature_cols = features , label_cols = target ) . get_loader ( batch_size = 32 , num_workers = 0 , shuffle = True ) valid_dl = SimpleDataloader . tabular_data_from_df ( df = valid_df , feature_cols = features , label_cols = target ) . get_loader ( batch_size = 32 , num_workers = 0 , shuffle = False )","title":"Creating the dataloaders"},{"location":"Tutorials/tabular_data/#defining-metrics-and-callbacks","text":"metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False , threshold = 0.6 )] callbacks = [ cbs . EarlyStopping ( monitor = \"accuracy\" , patience = 5 ), cbs . ModelCheckpoint ( monitor = \"accuracy\" ), ]","title":"Defining metrics and callbacks."},{"location":"Tutorials/tabular_data/#setting-up-the-experiment","text":"exp = Experiment ( num_epochs = 10 , save_dir = \"./models\" , model_name = \"tabular_cls.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) exp . compile_experiment ( model = model , optimizer = \"Adam\" , optimizer_params = dict ( lr = 3e-4 ), callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 2 ), criterion = crit . BCEWithLogitsFlat , metrics = metric_list , main_metric = \"accuracy\" , )","title":"Setting up the Experiment"},{"location":"Tutorials/tabular_data/#run-the-experiment","text":"exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) Here is a snapshot of how progress bar looks(Same as fastai, since we are using fastprogess). Notebook is available in examples folder.","title":"Run the experiment."},{"location":"Tutorials/text_classification/","text":"Text Classification with torchflare Let's learn how to use tinybert and torchflare for text classification. Dataset: https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format Importing Libraries import numpy as np import pandas as pd import torch import torch.nn as nn import transformers import torchflare.callbacks as cbs import torchflare.metrics as metrics import torchflare.criterion as crit from torchflare.experiments import Experiment from torchflare.datasets import SimpleDataloader Reading the data train_df = pd . read_csv ( 'Train.csv' ) valid_df = pd . read_csv ( 'Valid.csv' ) test_df = pd . read_csv ( 'Test.csv' ) Defining Model class Model ( torch . nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . bert = transformers . BertModel . from_pretrained ( \"prajjwal1/bert-tiny\" , return_dict = False ) self . bert_drop = nn . Dropout ( 0.3 ) self . out = nn . Linear ( 128 , 1 ) def forward ( self , input_ids , attention_mask , token_type_ids ): _ , o_2 = self . bert ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) b_o = self . bert_drop ( o_2 ) output = self . out ( b_o ) return output model = Model () Creating the dataloaders tokenizer = transformers . AutoTokenizer . from_pretrained ( \"prajjwal1/bert-tiny\" ) train_dl = SimpleDataloader . text_data_from_df ( df = train_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 , shuffle = True ) valid_dl = SimpleDataloader . text_data_from_df ( df = valid_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 ) test_dl = SimpleDataloader . text_data_from_df ( df = test_df , input_col = 'text' , label_cols = None , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 , shuffle = False ) Defining callbacks, metrics and some params. metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = acc . handle (), patience = 5 ), cbs . ModelCheckpoint ( monitor = acc . handle ()), ] # I want to define some custom weight decay to model paramters. # We will use model_params as an argument in optimizer_params to tell torchflare that, hey we are using custom optimizer params for model. # If model_params arguments is not used, torchflare by default will use model.parameters() as default params to optimizer. param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] optimizer_parameters = [ { \"params\" : [ p for n , p in param_optimizer if not any ( nd in n for nd in no_decay ) ], \"weight_decay\" : 0.001 , }, { \"params\" : [ p for n , p in param_optimizer if any ( nd in n for nd in no_decay )], \"weight_decay\" : 0.0 , }, ] Setting up the Experiment exp = Experiment ( num_epochs = 10 , save_dir = \"./models\" , model_name = \"bert_cls.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) # Compiling the experiment exp . compile_experiment ( model = model , optimizer = \"Adam\" , optimizer_params = dict ( model_params = optimizer_params , lr = 3e-4 ), # used model_params argument for custom optimizer params. callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 5 ), criterion = crit . BCEWithLogitsFlat , # Using BCEWithLogitsFlat since I dont want to handle shapes my outputs and targets. metrics = metric_list , main_metric = \"accuracy\" , ) # Training the models. exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) Notebook is available in examples folder.","title":"Text Data"},{"location":"Tutorials/text_classification/#text-classification-with-torchflare","text":"Let's learn how to use tinybert and torchflare for text classification. Dataset: https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format","title":"Text Classification with torchflare"},{"location":"Tutorials/text_classification/#importing-libraries","text":"import numpy as np import pandas as pd import torch import torch.nn as nn import transformers import torchflare.callbacks as cbs import torchflare.metrics as metrics import torchflare.criterion as crit from torchflare.experiments import Experiment from torchflare.datasets import SimpleDataloader","title":"Importing Libraries"},{"location":"Tutorials/text_classification/#reading-the-data","text":"train_df = pd . read_csv ( 'Train.csv' ) valid_df = pd . read_csv ( 'Valid.csv' ) test_df = pd . read_csv ( 'Test.csv' )","title":"Reading the data"},{"location":"Tutorials/text_classification/#defining-model","text":"class Model ( torch . nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . bert = transformers . BertModel . from_pretrained ( \"prajjwal1/bert-tiny\" , return_dict = False ) self . bert_drop = nn . Dropout ( 0.3 ) self . out = nn . Linear ( 128 , 1 ) def forward ( self , input_ids , attention_mask , token_type_ids ): _ , o_2 = self . bert ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) b_o = self . bert_drop ( o_2 ) output = self . out ( b_o ) return output model = Model ()","title":"Defining Model"},{"location":"Tutorials/text_classification/#creating-the-dataloaders","text":"tokenizer = transformers . AutoTokenizer . from_pretrained ( \"prajjwal1/bert-tiny\" ) train_dl = SimpleDataloader . text_data_from_df ( df = train_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 , shuffle = True ) valid_dl = SimpleDataloader . text_data_from_df ( df = valid_df , input_col = 'text' , label_cols = 'label' , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 ) test_dl = SimpleDataloader . text_data_from_df ( df = test_df , input_col = 'text' , label_cols = None , tokenizer = tokenizer , max_len = 128 ) . get_loader ( batch_size = 16 , shuffle = False )","title":"Creating the dataloaders"},{"location":"Tutorials/text_classification/#defining-callbacks-metrics-and-some-params","text":"metric_list = [ metrics . Accuracy ( num_classes = 2 , multilabel = False )] callbacks = [ cbs . EarlyStopping ( monitor = acc . handle (), patience = 5 ), cbs . ModelCheckpoint ( monitor = acc . handle ()), ] # I want to define some custom weight decay to model paramters. # We will use model_params as an argument in optimizer_params to tell torchflare that, hey we are using custom optimizer params for model. # If model_params arguments is not used, torchflare by default will use model.parameters() as default params to optimizer. param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] param_optimizer = list ( model . named_parameters ()) no_decay = [ \"bias\" , \"LayerNorm.bias\" ] optimizer_parameters = [ { \"params\" : [ p for n , p in param_optimizer if not any ( nd in n for nd in no_decay ) ], \"weight_decay\" : 0.001 , }, { \"params\" : [ p for n , p in param_optimizer if any ( nd in n for nd in no_decay )], \"weight_decay\" : 0.0 , }, ]","title":"Defining callbacks, metrics and some params."},{"location":"Tutorials/text_classification/#setting-up-the-experiment","text":"exp = Experiment ( num_epochs = 10 , save_dir = \"./models\" , model_name = \"bert_cls.bin\" , fp16 = False , using_batch_mixers = False , device = \"cuda\" , compute_train_metrics = True , seed = 42 , ) # Compiling the experiment exp . compile_experiment ( model = model , optimizer = \"Adam\" , optimizer_params = dict ( model_params = optimizer_params , lr = 3e-4 ), # used model_params argument for custom optimizer params. callbacks = callbacks , scheduler = \"ReduceLROnPlateau\" , scheduler_params = dict ( mode = \"max\" , patience = 5 ), criterion = crit . BCEWithLogitsFlat , # Using BCEWithLogitsFlat since I dont want to handle shapes my outputs and targets. metrics = metric_list , main_metric = \"accuracy\" , ) # Training the models. exp . run_experiment ( train_dl = train_dl , valid_dl = valid_dl ) Notebook is available in examples folder.","title":"Setting up the Experiment"},{"location":"batch_mixers/batch_mixer/","text":"Implementations of mixup , cutmix. Functions cutmix ( batch , alpha = 1.0 ) Function to perform cutmix. Cutmix: https://arxiv.org/abs/1905.04899 Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor] Tuple containing the data and targets. required alpha float beta distribution a=b parameters. 1.0 Returns: Type Description Tuple Image and targets. Source code in torchflare/batch_mixers/mixers.py def cutmix ( batch : Tuple [ torch . Tensor , torch . Tensor ], alpha : float = 1.0 ) -> Tuple : \"\"\"Function to perform cutmix. Cutmix: <https://arxiv.org/abs/1905.04899> Args: batch : Tuple containing the data and targets. alpha : beta distribution a=b parameters. Returns: Image and targets. \"\"\" data , targets = batch indices = torch . randperm ( data . size ( 0 )) shuffled_data = data [ indices ] shuffled_targets = targets [ indices ] lam = np . random . beta ( alpha , alpha ) if alpha > 0 else 1 x0 , x1 , y0 , y1 = random_bbox ( data , lam ) data [:, :, y0 : y1 , x0 : x1 ] = shuffled_data [:, :, y0 : y1 , x0 : x1 ] targets = ( targets , shuffled_targets , lam ) return data , targets get_collate_fn ( mixer_name , alpha ) Method to create collate_fn for dataloader. Parameters: Name Type Description Default mixer_name str The name of the batch_mixer. required alpha float beta distribution a=b parameters. required Returns: Type Description Callable The collate_fn for the respective special augmentation. Note aug_name must be one of cutmix , mixup Source code in torchflare/batch_mixers/mixers.py def get_collate_fn ( mixer_name : str , alpha : float ) -> Callable : \"\"\"Method to create collate_fn for dataloader. Args: mixer_name: The name of the batch_mixer. alpha: beta distribution a=b parameters. Returns: The collate_fn for the respective special augmentation. Note: aug_name must be one of cutmix , mixup \"\"\" fn = cutmix if mixer_name == \"cutmix\" else mixup collate_fn = CustomCollate ( alpha = alpha , mixer = fn ) return collate_fn mixup ( batch , alpha = 1.0 ) Function to mixup data. Mixup: https://arxiv.org/abs/1710.09412 Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor] Tuple containing the data and targets. required alpha float beta distribution a=b parameters. 1.0 Returns: Type Description Tuple The mixed image and targets. Source code in torchflare/batch_mixers/mixers.py def mixup ( batch : Tuple [ torch . Tensor , torch . Tensor ], alpha : float = 1.0 ) -> Tuple : \"\"\"Function to mixup data. Mixup: <https://arxiv.org/abs/1710.09412> Args: batch : Tuple containing the data and targets. alpha : beta distribution a=b parameters. Returns: The mixed image and targets. \"\"\" data , targets = batch lam = np . random . beta ( alpha , alpha ) if alpha > 0 else 1 indices = torch . randperm ( data . shape [ 0 ]) mixed_data = lam * data + ( 1 - lam ) * data [ indices , :] target_a , target_b = targets , targets [ indices ] targets = ( target_a , target_b , lam ) return mixed_data , targets","title":"Batch Mixers"},{"location":"batch_mixers/batch_mixer/#torchflare.batch_mixers.mixers-functions","text":"","title":"Functions"},{"location":"batch_mixers/batch_mixer/#torchflare.batch_mixers.mixers.cutmix","text":"Function to perform cutmix. Cutmix: https://arxiv.org/abs/1905.04899 Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor] Tuple containing the data and targets. required alpha float beta distribution a=b parameters. 1.0 Returns: Type Description Tuple Image and targets. Source code in torchflare/batch_mixers/mixers.py def cutmix ( batch : Tuple [ torch . Tensor , torch . Tensor ], alpha : float = 1.0 ) -> Tuple : \"\"\"Function to perform cutmix. Cutmix: <https://arxiv.org/abs/1905.04899> Args: batch : Tuple containing the data and targets. alpha : beta distribution a=b parameters. Returns: Image and targets. \"\"\" data , targets = batch indices = torch . randperm ( data . size ( 0 )) shuffled_data = data [ indices ] shuffled_targets = targets [ indices ] lam = np . random . beta ( alpha , alpha ) if alpha > 0 else 1 x0 , x1 , y0 , y1 = random_bbox ( data , lam ) data [:, :, y0 : y1 , x0 : x1 ] = shuffled_data [:, :, y0 : y1 , x0 : x1 ] targets = ( targets , shuffled_targets , lam ) return data , targets","title":"cutmix()"},{"location":"batch_mixers/batch_mixer/#torchflare.batch_mixers.mixers.get_collate_fn","text":"Method to create collate_fn for dataloader. Parameters: Name Type Description Default mixer_name str The name of the batch_mixer. required alpha float beta distribution a=b parameters. required Returns: Type Description Callable The collate_fn for the respective special augmentation. Note aug_name must be one of cutmix , mixup Source code in torchflare/batch_mixers/mixers.py def get_collate_fn ( mixer_name : str , alpha : float ) -> Callable : \"\"\"Method to create collate_fn for dataloader. Args: mixer_name: The name of the batch_mixer. alpha: beta distribution a=b parameters. Returns: The collate_fn for the respective special augmentation. Note: aug_name must be one of cutmix , mixup \"\"\" fn = cutmix if mixer_name == \"cutmix\" else mixup collate_fn = CustomCollate ( alpha = alpha , mixer = fn ) return collate_fn","title":"get_collate_fn()"},{"location":"batch_mixers/batch_mixer/#torchflare.batch_mixers.mixers.mixup","text":"Function to mixup data. Mixup: https://arxiv.org/abs/1710.09412 Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor] Tuple containing the data and targets. required alpha float beta distribution a=b parameters. 1.0 Returns: Type Description Tuple The mixed image and targets. Source code in torchflare/batch_mixers/mixers.py def mixup ( batch : Tuple [ torch . Tensor , torch . Tensor ], alpha : float = 1.0 ) -> Tuple : \"\"\"Function to mixup data. Mixup: <https://arxiv.org/abs/1710.09412> Args: batch : Tuple containing the data and targets. alpha : beta distribution a=b parameters. Returns: The mixed image and targets. \"\"\" data , targets = batch lam = np . random . beta ( alpha , alpha ) if alpha > 0 else 1 indices = torch . randperm ( data . shape [ 0 ]) mixed_data = lam * data + ( 1 - lam ) * data [ indices , :] target_a , target_b = targets , targets [ indices ] targets = ( target_a , target_b , lam ) return mixed_data , targets","title":"mixup()"},{"location":"callbacks/early_stopping/","text":"Implementation of Early Stopping Callback. Methods __init__ ( self , monitor = 'val_loss' , patience = 5 , mode = 'min' , min_delta = 1e-07 ) special Constructor for EarlyStopping class. Parameters: Name Type Description Default monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' patience int Number of epochs with no improvement after which training will be stopped. 5 mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. 'min' min_delta float Minimum change in the monitored quantity to qualify as an improvement. 1e-07 EarlyStopping will only use the values of metrics/loss obtained on validation set. Source code in torchflare/callbacks/early_stopping.py def __init__ ( self , monitor : str = \"val_loss\" , patience : int = 5 , mode : str = \"min\" , min_delta : float = 1e-7 , ): \"\"\"Constructor for EarlyStopping class. Args: monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. patience: Number of epochs with no improvement after which training will be stopped. mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. min_delta: Minimum change in the monitored quantity to qualify as an improvement. Note: EarlyStopping will only use the values of metrics/loss obtained on validation set. \"\"\" super ( EarlyStopping , self ) . __init__ ( order = CallbackOrder . STOPPING ) if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor self . patience = patience self . mode = mode self . min_delta = min_delta self . stopping_counter = 0 self . best_score = None self . improvement = None if self . mode == \"min\" : self . improvement = lambda score , best_score : score <= ( best_score - self . min_delta ) else : self . improvement = lambda score , best_score : score >= ( best_score + self . min_delta ) self . stopping_counter = 0 epoch_end ( self , epoch , logs ) Function which will determine when to stop the training depending on the score. Parameters: Name Type Description Default logs Dict A dictionary containing metrics and loss values. required epoch int The current epoch required Source code in torchflare/callbacks/early_stopping.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"Function which will determine when to stop the training depending on the score. Args: logs: A dictionary containing metrics and loss values. epoch : The current epoch \"\"\" epoch_score = logs . get ( self . monitor ) if self . best_score is None or self . improvement ( score = epoch_score , best_score = self . best_score ): self . best_score = epoch_score else : self . stopping_counter += 1 if self . stopping_counter >= self . patience : print ( \"Early Stopping !\" ) self . exp . stop_training = True Examples import torchflare.callbacks as cbs early_stop = cbs . EarlyStopping ( monitor = \"val_accuracy\" , patience = 5 , mode = \"max\" )","title":"EarlyStopping"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping-methods","text":"","title":"Methods"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping.__init__","text":"Constructor for EarlyStopping class. Parameters: Name Type Description Default monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' patience int Number of epochs with no improvement after which training will be stopped. 5 mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. 'min' min_delta float Minimum change in the monitored quantity to qualify as an improvement. 1e-07 EarlyStopping will only use the values of metrics/loss obtained on validation set. Source code in torchflare/callbacks/early_stopping.py def __init__ ( self , monitor : str = \"val_loss\" , patience : int = 5 , mode : str = \"min\" , min_delta : float = 1e-7 , ): \"\"\"Constructor for EarlyStopping class. Args: monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. patience: Number of epochs with no improvement after which training will be stopped. mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing.In \"max\" mode it will stop when the quantity monitored has stopped increasing. min_delta: Minimum change in the monitored quantity to qualify as an improvement. Note: EarlyStopping will only use the values of metrics/loss obtained on validation set. \"\"\" super ( EarlyStopping , self ) . __init__ ( order = CallbackOrder . STOPPING ) if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor self . patience = patience self . mode = mode self . min_delta = min_delta self . stopping_counter = 0 self . best_score = None self . improvement = None if self . mode == \"min\" : self . improvement = lambda score , best_score : score <= ( best_score - self . min_delta ) else : self . improvement = lambda score , best_score : score >= ( best_score + self . min_delta ) self . stopping_counter = 0","title":"__init__()"},{"location":"callbacks/early_stopping/#torchflare.callbacks.early_stopping.EarlyStopping.epoch_end","text":"Function which will determine when to stop the training depending on the score. Parameters: Name Type Description Default logs Dict A dictionary containing metrics and loss values. required epoch int The current epoch required Source code in torchflare/callbacks/early_stopping.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"Function which will determine when to stop the training depending on the score. Args: logs: A dictionary containing metrics and loss values. epoch : The current epoch \"\"\" epoch_score = logs . get ( self . monitor ) if self . best_score is None or self . improvement ( score = epoch_score , best_score = self . best_score ): self . best_score = epoch_score else : self . stopping_counter += 1 if self . stopping_counter >= self . patience : print ( \"Early Stopping !\" ) self . exp . stop_training = True","title":"epoch_end()"},{"location":"callbacks/early_stopping/#examples","text":"import torchflare.callbacks as cbs early_stop = cbs . EarlyStopping ( monitor = \"val_accuracy\" , patience = 5 , mode = \"max\" )","title":"Examples"},{"location":"callbacks/model_checkpoint/","text":"Callback for Checkpointing your model. Methods __init__ ( self , mode = 'min' , monitor = 'val_loss' ) special Constructor for ModelCheckpoint class. Parameters: Name Type Description Default mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. 'min' monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values : 1 ) ' model_state_dict ' : The state dictionary of model 2 ) ' optimizer_state_dict ' : The state dictionary of optimizer 3 ) ' scheduler_state_dict ' : The state dictionary of scheduler . ( If used ) 4 ) ' Epoch ' : The epoch at which model was saved . Model checkpoint will be saved based on the values of metrics / loss obtained from validation set . Source code in torchflare/callbacks/model_checkpoint.py def __init__ ( self , mode : str = \"min\" , monitor : str = \"val_loss\" ): \"\"\"Constructor for ModelCheckpoint class. Args: mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. Note: ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values: 1) 'model_state_dict' : The state dictionary of model 2) 'optimizer_state_dict' : The state dictionary of optimizer 3) 'scheduler_state_dict' : The state dictionary of scheduler.(If used) 4) 'Epoch' : The epoch at which model was saved. Model checkpoint will be saved based on the values of metrics/loss obtained from validation set. \"\"\" super ( ModelCheckpoint , self ) . __init__ ( order = CallbackOrder . INTERNAL ) self . mode = mode self . eps = 1e-7 if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor if self . mode == \"max\" : self . best_val = - np . inf self . improvement = lambda val , best_val : val >= best_val + self . eps else : self . best_val = np . inf self . improvement = lambda val , best_val : val <= best_val + self . eps checkpoint ( self , epoch ) Method to save the state dictionaries of model, optimizer,etc. Parameters: Name Type Description Default epoch int The epoch at which model is saved. required Source code in torchflare/callbacks/model_checkpoint.py def checkpoint ( self , epoch : int ): \"\"\"Method to save the state dictionaries of model, optimizer,etc. Args: epoch : The epoch at which model is saved. \"\"\" if self . exp . scheduler_stepper is not None : torch . save ( { \"model_state_dict\" : self . exp . model . state_dict (), \"optimizer_state_dict\" : self . exp . optimizer . state_dict (), \"scheduler_state_dict\" : self . exp . scheduler_stepper . scheduler . state_dict (), \"Epoch\" : epoch , }, self . exp . path , ) else : torch . save ( { \"model_state_dict\" : self . exp . model . state_dict (), \"optimizer_state_dict\" : self . exp . optimizer . state_dict (), \"Epoch\" : epoch , }, self . exp . path , ) epoch_end ( self , epoch , logs ) Method to save best model depending on the monitored quantity. Parameters: Name Type Description Default epoch int The current epoch. required logs Dict A dictionary containing metrics and loss values. required Source code in torchflare/callbacks/model_checkpoint.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"Method to save best model depending on the monitored quantity. Args: epoch: The current epoch. logs: A dictionary containing metrics and loss values. \"\"\" val = logs . get ( self . monitor ) if self . improvement ( val = val , best_val = self . best_val ): self . checkpoint ( epoch = epoch ) Examples import torchflare.callbacks as cbs model_ckpt = cbs . ModelCheckpoint ( monitor = \"val_accuracy\" , mode = \"max\" )","title":"ModelCheckpoint"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint-methods","text":"","title":"Methods"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint.__init__","text":"Constructor for ModelCheckpoint class. Parameters: Name Type Description Default mode str One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. 'min' monitor str The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. 'val_loss' ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values : 1 ) ' model_state_dict ' : The state dictionary of model 2 ) ' optimizer_state_dict ' : The state dictionary of optimizer 3 ) ' scheduler_state_dict ' : The state dictionary of scheduler . ( If used ) 4 ) ' Epoch ' : The epoch at which model was saved . Model checkpoint will be saved based on the values of metrics / loss obtained from validation set . Source code in torchflare/callbacks/model_checkpoint.py def __init__ ( self , mode : str = \"min\" , monitor : str = \"val_loss\" ): \"\"\"Constructor for ModelCheckpoint class. Args: mode: One of {\"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing in \"max\" mode it will stop when the quantity monitored has stopped increasing. monitor: The quantity to be monitored. (Default : val_loss) If you want to monitor other metric just pass in the name of the metric. Note: ModelCheckpoint will save state_dictionaries for model , optimizer , scheduler and the value of epoch with following key values: 1) 'model_state_dict' : The state dictionary of model 2) 'optimizer_state_dict' : The state dictionary of optimizer 3) 'scheduler_state_dict' : The state dictionary of scheduler.(If used) 4) 'Epoch' : The epoch at which model was saved. Model checkpoint will be saved based on the values of metrics/loss obtained from validation set. \"\"\" super ( ModelCheckpoint , self ) . __init__ ( order = CallbackOrder . INTERNAL ) self . mode = mode self . eps = 1e-7 if \"val_\" not in monitor : self . monitor = \"val_\" + monitor else : self . monitor = monitor if self . mode == \"max\" : self . best_val = - np . inf self . improvement = lambda val , best_val : val >= best_val + self . eps else : self . best_val = np . inf self . improvement = lambda val , best_val : val <= best_val + self . eps","title":"__init__()"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint.checkpoint","text":"Method to save the state dictionaries of model, optimizer,etc. Parameters: Name Type Description Default epoch int The epoch at which model is saved. required Source code in torchflare/callbacks/model_checkpoint.py def checkpoint ( self , epoch : int ): \"\"\"Method to save the state dictionaries of model, optimizer,etc. Args: epoch : The epoch at which model is saved. \"\"\" if self . exp . scheduler_stepper is not None : torch . save ( { \"model_state_dict\" : self . exp . model . state_dict (), \"optimizer_state_dict\" : self . exp . optimizer . state_dict (), \"scheduler_state_dict\" : self . exp . scheduler_stepper . scheduler . state_dict (), \"Epoch\" : epoch , }, self . exp . path , ) else : torch . save ( { \"model_state_dict\" : self . exp . model . state_dict (), \"optimizer_state_dict\" : self . exp . optimizer . state_dict (), \"Epoch\" : epoch , }, self . exp . path , )","title":"checkpoint()"},{"location":"callbacks/model_checkpoint/#torchflare.callbacks.model_checkpoint.ModelCheckpoint.epoch_end","text":"Method to save best model depending on the monitored quantity. Parameters: Name Type Description Default epoch int The current epoch. required logs Dict A dictionary containing metrics and loss values. required Source code in torchflare/callbacks/model_checkpoint.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"Method to save best model depending on the monitored quantity. Args: epoch: The current epoch. logs: A dictionary containing metrics and loss values. \"\"\" val = logs . get ( self . monitor ) if self . improvement ( val = val , best_val = self . best_val ): self . checkpoint ( epoch = epoch )","title":"epoch_end()"},{"location":"callbacks/model_checkpoint/#examples","text":"import torchflare.callbacks as cbs model_ckpt = cbs . ModelCheckpoint ( monitor = \"val_accuracy\" , mode = \"max\" )","title":"Examples"},{"location":"callbacks/logging/comet_logger/","text":"Callback to log your metrics and loss values to Comet to track your experiments. For more information about Comet look at Comet.ml Methods __init__ ( self , api_token , params , project_name , workspace , tags ) special Constructor for CometLogger class. Parameters: Name Type Description Default api_token str Your API key obtained from comet.ml required params dict The hyperparameters for your model and experiment as a dictionary required project_name str Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. required workspace str Attach an experiment to a project that belongs to this workspace required tags List[str] List of strings. required Source code in torchflare/callbacks/logging/comet_logger.py def __init__ ( self , api_token : str , params : dict , project_name : str , workspace : str , tags : List [ str ], ): \"\"\"Constructor for CometLogger class. Args: api_token: Your API key obtained from comet.ml params: The hyperparameters for your model and experiment as a dictionary project_name: Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. workspace: Attach an experiment to a project that belongs to this workspace tags: List of strings. \"\"\" super ( CometLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . api_token = api_token self . project_name = project_name self . workspace = workspace self . params = params self . tags = tags self . experiment = comet_ml . Experiment ( project_name = self . project_name , api_key = self . api_token , workspace = self . workspace , log_code = False , display_summary_level = 0 , ) if self . tags is not None : self . experiment . add_tags ( self . tags ) if self . params is not None : self . experiment . log_parameters ( self . params ) epoch_end ( self , epoch , logs ) Function to log your metrics and values at the end of very epoch. Parameters: Name Type Description Default logs dict A dictionary containing metrics and loss values. required epoch int The current epoch required Source code in torchflare/callbacks/logging/comet_logger.py def epoch_end ( self , epoch : int , logs : dict ): \"\"\"Function to log your metrics and values at the end of very epoch. Args: logs : A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" _ = logs . pop ( \"Time\" ) self . experiment . log_metrics ( logs , step = epoch ) experiment_end ( self ) Function to close the experiment when training ends. Source code in torchflare/callbacks/logging/comet_logger.py def experiment_end ( self ): \"\"\"Function to close the experiment when training ends.\"\"\" self . experiment . end () Examples from torchflare.callbacks import CometLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = CometLogger ( project_name = \"experiment_10\" , workspace = \"username\" , params = params , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"CometLogger"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger.__init__","text":"Constructor for CometLogger class. Parameters: Name Type Description Default api_token str Your API key obtained from comet.ml required params dict The hyperparameters for your model and experiment as a dictionary required project_name str Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. required workspace str Attach an experiment to a project that belongs to this workspace required tags List[str] List of strings. required Source code in torchflare/callbacks/logging/comet_logger.py def __init__ ( self , api_token : str , params : dict , project_name : str , workspace : str , tags : List [ str ], ): \"\"\"Constructor for CometLogger class. Args: api_token: Your API key obtained from comet.ml params: The hyperparameters for your model and experiment as a dictionary project_name: Send your experiment to a specific project. Otherwise, will be sent to Uncategorized Experiments. workspace: Attach an experiment to a project that belongs to this workspace tags: List of strings. \"\"\" super ( CometLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . api_token = api_token self . project_name = project_name self . workspace = workspace self . params = params self . tags = tags self . experiment = comet_ml . Experiment ( project_name = self . project_name , api_key = self . api_token , workspace = self . workspace , log_code = False , display_summary_level = 0 , ) if self . tags is not None : self . experiment . add_tags ( self . tags ) if self . params is not None : self . experiment . log_parameters ( self . params )","title":"__init__()"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger.epoch_end","text":"Function to log your metrics and values at the end of very epoch. Parameters: Name Type Description Default logs dict A dictionary containing metrics and loss values. required epoch int The current epoch required Source code in torchflare/callbacks/logging/comet_logger.py def epoch_end ( self , epoch : int , logs : dict ): \"\"\"Function to log your metrics and values at the end of very epoch. Args: logs : A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" _ = logs . pop ( \"Time\" ) self . experiment . log_metrics ( logs , step = epoch )","title":"epoch_end()"},{"location":"callbacks/logging/comet_logger/#torchflare.callbacks.logging.comet_logger.CometLogger.experiment_end","text":"Function to close the experiment when training ends. Source code in torchflare/callbacks/logging/comet_logger.py def experiment_end ( self ): \"\"\"Function to close the experiment when training ends.\"\"\" self . experiment . end ()","title":"experiment_end()"},{"location":"callbacks/logging/comet_logger/#examples","text":"from torchflare.callbacks import CometLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = CometLogger ( project_name = \"experiment_10\" , workspace = \"username\" , params = params , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"Examples"},{"location":"callbacks/logging/neptune_logger/","text":"Callback to log your metrics and loss values to Neptune to track your experiments. For more information about Neptune take a look at Neptune Methods __init__ ( self , project_dir , api_token , params = None , experiment_name = None , tags = None ) special Constructor for NeptuneLogger Class. Parameters: Name Type Description Default project_dir str The qualified name of a project in a form of namespace/project_name required params dict he hyperparameters for your model and experiment as a dictionary None experiment_name str The name of the experiment None api_token str User\u2019s API token required tags List[str] List of strings. None Source code in torchflare/callbacks/logging/neptune_logger.py def __init__ ( self , project_dir : str , api_token : str , params : dict = None , experiment_name : str = None , tags : List [ str ] = None , ): \"\"\"Constructor for NeptuneLogger Class. Args: project_dir: The qualified name of a project in a form of namespace/project_name params: he hyperparameters for your model and experiment as a dictionary experiment_name: The name of the experiment api_token: User\u2019s API token tags: List of strings. \"\"\" super ( NeptuneLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . project_dir = project_dir self . api_token = api_token self . params = params self . tags = tags self . experiment_name = experiment_name self . experiment = neptune . init ( project = self . project_dir , api_token = self . api_token , tags = self . tags , name = self . experiment_name ) self . experiment [ \"params\" ] = self . params epoch_end ( self , epoch , logs ) Method to log metrics and values at the end of very epoch. Parameters: Name Type Description Default logs A dictionary containing metrics and loss values. required epoch The current epoch required Source code in torchflare/callbacks/logging/neptune_logger.py def epoch_end ( self , epoch , logs ): \"\"\"Method to log metrics and values at the end of very epoch. Args: logs : A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" for key , value in logs . items (): if not isinstance ( value , str ): self . _log_metrics ( name = key , value = value , epoch = epoch ) experiment_end ( self ) Method to end experiment after training is done. Source code in torchflare/callbacks/logging/neptune_logger.py def experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . stop () Examples from torchflare.callbacks import NeptuneLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = NeptuneLogger ( project_dir = \"username/Experiments\" , params = params , experiment_name = \"Experiment_10\" , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"NeptuneLogger"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger.__init__","text":"Constructor for NeptuneLogger Class. Parameters: Name Type Description Default project_dir str The qualified name of a project in a form of namespace/project_name required params dict he hyperparameters for your model and experiment as a dictionary None experiment_name str The name of the experiment None api_token str User\u2019s API token required tags List[str] List of strings. None Source code in torchflare/callbacks/logging/neptune_logger.py def __init__ ( self , project_dir : str , api_token : str , params : dict = None , experiment_name : str = None , tags : List [ str ] = None , ): \"\"\"Constructor for NeptuneLogger Class. Args: project_dir: The qualified name of a project in a form of namespace/project_name params: he hyperparameters for your model and experiment as a dictionary experiment_name: The name of the experiment api_token: User\u2019s API token tags: List of strings. \"\"\" super ( NeptuneLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . project_dir = project_dir self . api_token = api_token self . params = params self . tags = tags self . experiment_name = experiment_name self . experiment = neptune . init ( project = self . project_dir , api_token = self . api_token , tags = self . tags , name = self . experiment_name ) self . experiment [ \"params\" ] = self . params","title":"__init__()"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger.epoch_end","text":"Method to log metrics and values at the end of very epoch. Parameters: Name Type Description Default logs A dictionary containing metrics and loss values. required epoch The current epoch required Source code in torchflare/callbacks/logging/neptune_logger.py def epoch_end ( self , epoch , logs ): \"\"\"Method to log metrics and values at the end of very epoch. Args: logs : A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" for key , value in logs . items (): if not isinstance ( value , str ): self . _log_metrics ( name = key , value = value , epoch = epoch )","title":"epoch_end()"},{"location":"callbacks/logging/neptune_logger/#torchflare.callbacks.logging.neptune_logger.NeptuneLogger.experiment_end","text":"Method to end experiment after training is done. Source code in torchflare/callbacks/logging/neptune_logger.py def experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . stop ()","title":"experiment_end()"},{"location":"callbacks/logging/neptune_logger/#examples","text":"from torchflare.callbacks import NeptuneLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = NeptuneLogger ( project_dir = \"username/Experiments\" , params = params , experiment_name = \"Experiment_10\" , tags = [ \"Experiment\" , \"fold_0\" ], api_token = \"your_secret_api_token\" , )","title":"Examples"},{"location":"callbacks/logging/tensorboard_logger/","text":"Callback to use Tensorboard to log your metrics and losses. Methods __init__ ( self , log_dir ) special Constructor for TensorboardLogger class. Parameters: Name Type Description Default log_dir str The directory where you want to save your experiments. required Source code in torchflare/callbacks/logging/tensorboard_logger.py def __init__ ( self , log_dir : str ): \"\"\"Constructor for TensorboardLogger class. Args: log_dir: The directory where you want to save your experiments. \"\"\" super ( TensorboardLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . log_dir = log_dir self . _experiment = SummaryWriter ( log_dir = self . log_dir ) epoch_end ( self , epoch , logs ) Method to log metrics and values at the end of very epoch. Parameters: Name Type Description Default logs dict A dictionary containing metrics and loss values. required epoch int The current epoch required Source code in torchflare/callbacks/logging/tensorboard_logger.py def epoch_end ( self , epoch : int , logs : dict ): \"\"\"Method to log metrics and values at the end of very epoch. Args: logs: A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" for key , value in logs . items (): if not isinstance ( value , str ): self . _experiment . add_scalar ( tag = key , scalar_value = value , global_step = epoch ) experiment_end ( self ) Method to end experiment after training is done. Source code in torchflare/callbacks/logging/tensorboard_logger.py def experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . _experiment . close ()","title":"TensorboardLogger"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger.__init__","text":"Constructor for TensorboardLogger class. Parameters: Name Type Description Default log_dir str The directory where you want to save your experiments. required Source code in torchflare/callbacks/logging/tensorboard_logger.py def __init__ ( self , log_dir : str ): \"\"\"Constructor for TensorboardLogger class. Args: log_dir: The directory where you want to save your experiments. \"\"\" super ( TensorboardLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . log_dir = log_dir self . _experiment = SummaryWriter ( log_dir = self . log_dir )","title":"__init__()"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger.epoch_end","text":"Method to log metrics and values at the end of very epoch. Parameters: Name Type Description Default logs dict A dictionary containing metrics and loss values. required epoch int The current epoch required Source code in torchflare/callbacks/logging/tensorboard_logger.py def epoch_end ( self , epoch : int , logs : dict ): \"\"\"Method to log metrics and values at the end of very epoch. Args: logs: A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" for key , value in logs . items (): if not isinstance ( value , str ): self . _experiment . add_scalar ( tag = key , scalar_value = value , global_step = epoch )","title":"epoch_end()"},{"location":"callbacks/logging/tensorboard_logger/#torchflare.callbacks.logging.tensorboard_logger.TensorboardLogger.experiment_end","text":"Method to end experiment after training is done. Source code in torchflare/callbacks/logging/tensorboard_logger.py def experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . _experiment . close ()","title":"experiment_end()"},{"location":"callbacks/logging/wandb_logger/","text":"Callback to log your metrics and loss values to wandb platform. For more information about wandb take a look at Weights and Biases Methods __init__ ( self , project , entity , name = None , config = None , tags = None , notes = None , directory = None ) special Constructor of WandbLogger. Parameters: Name Type Description Default project str The name of the project where you're sending the new run required entity str An entity is a username or team name where you're sending runs. required name str A short display name for this run None config Dict The hyperparameters for your model and experiment as a dictionary None tags List[str] List of strings. None directory str where to save wandb local run directory. If set to None it will use experiments save_dir argument. None notes Optional[str] A longer description of the run, like a -m commit message in git None Note set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log Source code in torchflare/callbacks/logging/wandb_logger.py def __init__ ( self , project : str , entity : str , name : str = None , config : Dict = None , tags : List [ str ] = None , notes : Optional [ str ] = None , directory : str = None , ): \"\"\"Constructor of WandbLogger. Args: project: The name of the project where you're sending the new run entity: An entity is a username or team name where you're sending runs. name: A short display name for this run config: The hyperparameters for your model and experiment as a dictionary tags: List of strings. directory: where to save wandb local run directory. If set to None it will use experiments save_dir argument. notes: A longer description of the run, like a -m commit message in git Note: set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log \"\"\" super ( WandbLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . experiment = wandb . init ( entity = entity , project = project , name = name , config = config , tags = tags , notes = notes , dir = directory ) epoch_end ( self , epoch , logs ) Method to log metrics and values at the end of very epoch. Parameters: Name Type Description Default logs A dictionary containing metrics and loss values. required epoch The current epoch required Source code in torchflare/callbacks/logging/wandb_logger.py def epoch_end ( self , epoch , logs ): \"\"\"Method to log metrics and values at the end of very epoch. Args: logs: A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" _ = logs . pop ( \"Time\" ) self . experiment . log ( logs ) experiment_end ( self ) Method to end experiment after training is done. Source code in torchflare/callbacks/logging/wandb_logger.py def experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . finish () Examples from torchflare.callbacks import WandbLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = WandbLogger ( project = \"Experiment\" , entity = \"username\" , name = \"Experiment_10\" , config = params , tags = [ \"Experiment\" , \"fold_0\" ], )","title":"WandbLogger"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger-methods","text":"","title":"Methods"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger.__init__","text":"Constructor of WandbLogger. Parameters: Name Type Description Default project str The name of the project where you're sending the new run required entity str An entity is a username or team name where you're sending runs. required name str A short display name for this run None config Dict The hyperparameters for your model and experiment as a dictionary None tags List[str] List of strings. None directory str where to save wandb local run directory. If set to None it will use experiments save_dir argument. None notes Optional[str] A longer description of the run, like a -m commit message in git None Note set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log Source code in torchflare/callbacks/logging/wandb_logger.py def __init__ ( self , project : str , entity : str , name : str = None , config : Dict = None , tags : List [ str ] = None , notes : Optional [ str ] = None , directory : str = None , ): \"\"\"Constructor of WandbLogger. Args: project: The name of the project where you're sending the new run entity: An entity is a username or team name where you're sending runs. name: A short display name for this run config: The hyperparameters for your model and experiment as a dictionary tags: List of strings. directory: where to save wandb local run directory. If set to None it will use experiments save_dir argument. notes: A longer description of the run, like a -m commit message in git Note: set os.environ['WANDB_SILENT'] = True to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log \"\"\" super ( WandbLogger , self ) . __init__ ( order = CallbackOrder . LOGGING ) self . experiment = wandb . init ( entity = entity , project = project , name = name , config = config , tags = tags , notes = notes , dir = directory )","title":"__init__()"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger.epoch_end","text":"Method to log metrics and values at the end of very epoch. Parameters: Name Type Description Default logs A dictionary containing metrics and loss values. required epoch The current epoch required Source code in torchflare/callbacks/logging/wandb_logger.py def epoch_end ( self , epoch , logs ): \"\"\"Method to log metrics and values at the end of very epoch. Args: logs: A dictionary containing metrics and loss values. epoch: The current epoch \"\"\" _ = logs . pop ( \"Time\" ) self . experiment . log ( logs )","title":"epoch_end()"},{"location":"callbacks/logging/wandb_logger/#torchflare.callbacks.logging.wandb_logger.WandbLogger.experiment_end","text":"Method to end experiment after training is done. Source code in torchflare/callbacks/logging/wandb_logger.py def experiment_end ( self ): \"\"\"Method to end experiment after training is done.\"\"\" self . experiment . finish ()","title":"experiment_end()"},{"location":"callbacks/logging/wandb_logger/#examples","text":"from torchflare.callbacks import WandbLogger params = { \"bs\" : 16 , \"lr\" : 0.3 } logger = WandbLogger ( project = \"Experiment\" , entity = \"username\" , name = \"Experiment_10\" , config = params , tags = [ \"Experiment\" , \"fold_0\" ], )","title":"Examples"},{"location":"callbacks/notifiers/discord_notifier/","text":"Class to Dispatch Training progress to your Discord Sever. Methods __init__ ( self , exp_name , webhook_url ) special Constructor method for DiscordNotifierCallback. Parameters: Name Type Description Default exp_name str The name of your experiment bot. (Can be anything) required webhook_url str The webhook url of your discord server/channel. required Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , exp_name : str , webhook_url : str ): \"\"\"Constructor method for DiscordNotifierCallback. Args: exp_name : The name of your experiment bot. (Can be anything) webhook_url : The webhook url of your discord server/channel. \"\"\" super ( DiscordNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . exp_name = exp_name self . webhook_url = webhook_url epoch_end ( self , epoch , logs ) This function will dispatch messages to your discord server/channel. Parameters: Name Type Description Default epoch int The current epoch required logs Dict a str, list of strings, dictionary required Source code in torchflare/callbacks/notifiers/message_notifiers.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"This function will dispatch messages to your discord server/channel. Args: epoch : The current epoch logs : a str, list of strings, dictionary \"\"\" logs = { \"Epoch\" : epoch , ** logs } data = { \"username\" : self . exp_name , \"embeds\" : [{ \"description\" : prepare_data ( logs )}], } response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) try : response . raise_for_status () except requests . exceptions . HTTPError as err : print ( err ) Examples import torchflare.callbacks as cbs discord_notif = cbs . DiscordNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" , exp_name = \"MODEL_RUN\" )","title":"DiscordNotifierCallback"},{"location":"callbacks/notifiers/discord_notifier/#torchflare.callbacks.notifiers.message_notifiers.DiscordNotifierCallback-methods","text":"","title":"Methods"},{"location":"callbacks/notifiers/discord_notifier/#torchflare.callbacks.notifiers.message_notifiers.DiscordNotifierCallback.__init__","text":"Constructor method for DiscordNotifierCallback. Parameters: Name Type Description Default exp_name str The name of your experiment bot. (Can be anything) required webhook_url str The webhook url of your discord server/channel. required Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , exp_name : str , webhook_url : str ): \"\"\"Constructor method for DiscordNotifierCallback. Args: exp_name : The name of your experiment bot. (Can be anything) webhook_url : The webhook url of your discord server/channel. \"\"\" super ( DiscordNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . exp_name = exp_name self . webhook_url = webhook_url","title":"__init__()"},{"location":"callbacks/notifiers/discord_notifier/#torchflare.callbacks.notifiers.message_notifiers.DiscordNotifierCallback.epoch_end","text":"This function will dispatch messages to your discord server/channel. Parameters: Name Type Description Default epoch int The current epoch required logs Dict a str, list of strings, dictionary required Source code in torchflare/callbacks/notifiers/message_notifiers.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"This function will dispatch messages to your discord server/channel. Args: epoch : The current epoch logs : a str, list of strings, dictionary \"\"\" logs = { \"Epoch\" : epoch , ** logs } data = { \"username\" : self . exp_name , \"embeds\" : [{ \"description\" : prepare_data ( logs )}], } response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) try : response . raise_for_status () except requests . exceptions . HTTPError as err : print ( err )","title":"epoch_end()"},{"location":"callbacks/notifiers/discord_notifier/#examples","text":"import torchflare.callbacks as cbs discord_notif = cbs . DiscordNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" , exp_name = \"MODEL_RUN\" )","title":"Examples"},{"location":"callbacks/notifiers/slack_notifier/","text":"Class to Dispatch Training progress to your Slack channel. Methods __init__ ( self , webhook_url ) special Constructor method for SlackNotifierCallback. Parameters: Name Type Description Default webhook_url str Slack webhook url required Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , webhook_url : str ): \"\"\"Constructor method for SlackNotifierCallback. Args: webhook_url : Slack webhook url \"\"\" super ( SlackNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . webhook_url = webhook_url epoch_end ( self , epoch , logs ) This function will dispatch messages to your Slack channel. Parameters: Name Type Description Default epoch int The current epoch required logs Dict a str, list of strings, dictionary required Exceptions: Type Description ValueError If connection to slack channel could not be established. Source code in torchflare/callbacks/notifiers/message_notifiers.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"This function will dispatch messages to your Slack channel. Args: epoch : The current epoch logs : a str, list of strings, dictionary Raises: ValueError: If connection to slack channel could not be established. \"\"\" logs = { \"Epoch\" : epoch , ** logs } data = { \"text\" : prepare_data ( logs )} response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) if response . status_code != 200 : raise ValueError ( \"Request to slack returned an error {} , the response is: \\n {} \" . format ( response . status_code , response . text ) ) Examples import torchflare.callbacks as cbs slack_notif = cbs . SlackNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" )","title":"SlackNotifierCallback"},{"location":"callbacks/notifiers/slack_notifier/#torchflare.callbacks.notifiers.message_notifiers.SlackNotifierCallback-methods","text":"","title":"Methods"},{"location":"callbacks/notifiers/slack_notifier/#torchflare.callbacks.notifiers.message_notifiers.SlackNotifierCallback.__init__","text":"Constructor method for SlackNotifierCallback. Parameters: Name Type Description Default webhook_url str Slack webhook url required Source code in torchflare/callbacks/notifiers/message_notifiers.py def __init__ ( self , webhook_url : str ): \"\"\"Constructor method for SlackNotifierCallback. Args: webhook_url : Slack webhook url \"\"\" super ( SlackNotifierCallback , self ) . __init__ ( order = CallbackOrder . EXTERNAL ) self . webhook_url = webhook_url","title":"__init__()"},{"location":"callbacks/notifiers/slack_notifier/#torchflare.callbacks.notifiers.message_notifiers.SlackNotifierCallback.epoch_end","text":"This function will dispatch messages to your Slack channel. Parameters: Name Type Description Default epoch int The current epoch required logs Dict a str, list of strings, dictionary required Exceptions: Type Description ValueError If connection to slack channel could not be established. Source code in torchflare/callbacks/notifiers/message_notifiers.py def epoch_end ( self , epoch : int , logs : Dict ): \"\"\"This function will dispatch messages to your Slack channel. Args: epoch : The current epoch logs : a str, list of strings, dictionary Raises: ValueError: If connection to slack channel could not be established. \"\"\" logs = { \"Epoch\" : epoch , ** logs } data = { \"text\" : prepare_data ( logs )} response = requests . post ( self . webhook_url , json . dumps ( data ), headers = { \"Content-Type\" : \"application/json\" }) if response . status_code != 200 : raise ValueError ( \"Request to slack returned an error {} , the response is: \\n {} \" . format ( response . status_code , response . text ) )","title":"epoch_end()"},{"location":"callbacks/notifiers/slack_notifier/#examples","text":"import torchflare.callbacks as cbs slack_notif = cbs . SlackNotifierCallback ( webhook_url = \"YOUR_SECRET_URL\" )","title":"Examples"},{"location":"criterion/cross_entropy/","text":"Implements variants for Cross Entropy loss. Classes LabelSmoothingCrossEntropy NLL loss with targets smoothing. Methods __init__ ( self , smoothing = 0.1 ) special Constructor method for LabelSmoothingCrossEntropy. Parameters: Name Type Description Default smoothing float targets smoothing factor 0.1 Exceptions: Type Description ValueError value error is raised if smoothing > 1.0. Source code in torchflare/criterion/cross_entropy.py def __init__ ( self , smoothing : float = 0.1 ): \"\"\"Constructor method for LabelSmoothingCrossEntropy. Args: smoothing : targets smoothing factor Raises: ValueError: value error is raised if smoothing > 1.0. \"\"\" super ( LabelSmoothingCrossEntropy , self ) . __init__ () if smoothing > 1.0 : raise ValueError ( \"Smoothing value must be less than 1.\" ) self . smoothing = smoothing self . confidence = 1.0 - smoothing forward ( self , logits , target ) Forward method. Parameters: Name Type Description Default logits Tensor Raw logits from the net. required target Tensor The targets. required Returns: Type Description Tensor The computed loss value. Source code in torchflare/criterion/cross_entropy.py def forward ( self , logits : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: logits: Raw logits from the net. target: The targets. Returns: The computed loss value. \"\"\" logprobs = F . log_softmax ( logits , dim =- 1 ) nll_loss = - logprobs . gather ( dim =- 1 , index = target . unsqueeze ( 1 )) nll_loss = nll_loss . squeeze ( 1 ) smooth_loss = - logprobs . mean ( dim =- 1 ) loss = self . confidence * nll_loss + self . smoothing * smooth_loss return loss . mean () SymmetricCE Pytorch Implementation of Symmetric Cross Entropy. Paper: https://arxiv.org/abs/1908.06112 Methods __init__ ( self , num_classes , alpha = 1.0 , beta = 1.0 ) special Constructor method for symmetric CE. Parameters: Name Type Description Default alpha float The alpha value for symmetricCE. 1.0 beta float The beta value for symmetricCE. 1.0 num_classes The number of classes. required Source code in torchflare/criterion/cross_entropy.py def __init__ ( self , num_classes , alpha : float = 1.0 , beta : float = 1.0 ): \"\"\"Constructor method for symmetric CE. Args: alpha: The alpha value for symmetricCE. beta: The beta value for symmetricCE. num_classes: The number of classes. \"\"\" super ( SymmetricCE , self ) . __init__ () self . alpha = alpha self . beta = beta self . num_classes = num_classes self . ce = nn . CrossEntropyLoss () forward ( self , logits , targets ) Forward method. Source code in torchflare/criterion/cross_entropy.py def forward ( self , logits : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method.\"\"\" ce = self . ce ( logits , targets ) logits = F . softmax ( logits , dim = 1 ) logits = torch . clamp ( logits , min = 1e-7 , max = 1.0 ) if logits . is_cuda : label_one_hot = torch . nn . functional . one_hot ( targets , self . num_classes ) . float () . cuda () else : label_one_hot = torch . nn . functional . one_hot ( targets , self . num_classes ) label_one_hot = torch . clamp ( label_one_hot , min = 1e-4 , max = 1.0 ) rce = - 1 * torch . sum ( logits * torch . log ( label_one_hot ), dim = 1 ) loss = self . alpha * ce + self . beta * rce . mean () return loss Functions BCEFlat ( x , y ) Same as F.binary_cross_entropy but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" x = torch . sigmoid ( x ) y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy ( x , y ) BCEWithLogitsFlat ( x , y ) Same as F.binary_cross_entropy_with_logits but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEWithLogitsFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy_with_logits but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy_with_logits ( x , y )","title":"Cross Entropy Losses"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy-classes","text":"","title":"Classes"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.LabelSmoothingCrossEntropy","text":"NLL loss with targets smoothing.","title":"LabelSmoothingCrossEntropy"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.LabelSmoothingCrossEntropy-methods","text":"","title":"Methods"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.SymmetricCE","text":"Pytorch Implementation of Symmetric Cross Entropy. Paper: https://arxiv.org/abs/1908.06112","title":"SymmetricCE"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.SymmetricCE-methods","text":"","title":"Methods"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy-functions","text":"","title":"Functions"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.BCEFlat","text":"Same as F.binary_cross_entropy but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" x = torch . sigmoid ( x ) y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy ( x , y )","title":"BCEFlat()"},{"location":"criterion/cross_entropy/#torchflare.criterion.cross_entropy.BCEWithLogitsFlat","text":"Same as F.binary_cross_entropy_with_logits but flattens the input and target. Parameters: Name Type Description Default x Tensor logits required y Tensor The corresponding targets. required Returns: Type Description Tensor The computed Loss Source code in torchflare/criterion/cross_entropy.py def BCEWithLogitsFlat ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : \"\"\"Same as F.binary_cross_entropy_with_logits but flattens the input and target. Args: x : logits y: The corresponding targets. Returns: The computed Loss \"\"\" y = y . view ( x . shape ) . type_as ( x ) return torch . nn . functional . binary_cross_entropy_with_logits ( x , y )","title":"BCEWithLogitsFlat()"},{"location":"criterion/focal_loss/","text":"Implements variants for Focal loss. Classes BCEFocalLoss Implementation of Focal Loss for Binary Classification Problems. Focal loss was proposed in Focal Loss for Dense Object Detection . Methods __init__ ( self , gamma = 0 , eps = 1e-07 , reduction = 'mean' ) special Constructor Method for FocalLoss class. Parameters: Name Type Description Default gamma The focal parameter. Defaults to 0. 0 eps Constant for computational stability. 1e-07 reduction The reduction parameter for Cross Entropy Loss. 'mean' Source code in torchflare/criterion/focal_loss.py def __init__ ( self , gamma = 0 , eps = 1e-7 , reduction = \"mean\" ): \"\"\"Constructor Method for FocalLoss class. Args: gamma : The focal parameter. Defaults to 0. eps : Constant for computational stability. reduction: The reduction parameter for Cross Entropy Loss. \"\"\" super ( BCEFocalLoss , self ) . __init__ () self . gamma = gamma self . reduction = reduction self . eps = eps self . bce = torch . nn . BCEWithLogitsLoss ( reduction = \"none\" ) forward ( self , logits , targets ) Forward method. Parameters: Name Type Description Default logits Tensor The raw logits from the network of shape (N,*) where C = number of classes , * = extra dims required targets Tensor The targets required Returns: Type Description Tensor The computed loss value Source code in torchflare/criterion/focal_loss.py def forward ( self , logits : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: logits: The raw logits from the network of shape (N,*) where C = number of classes , * = extra dims targets: The targets Returns: The computed loss value \"\"\" targets = targets . view ( logits . shape ) logp = self . bce ( logits , targets ) p = torch . exp ( - logp ) loss = ( 1 - p ) ** self . gamma * logp return loss . mean () if self . reduction == \"mean\" else loss . sum () if self . reduction == \"sum\" else loss FocalCosineLoss Implementation Focal cosine loss. Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble . Source : https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/203271 Methods __init__ ( self , alpha = 1 , gamma = 2 , xent = 0.1 , reduction = 'mean' ) special Constructor for FocalCosineLoss. Todo: Add documentation. Source code in torchflare/criterion/focal_loss.py def __init__ ( self , alpha : float = 1 , gamma : float = 2 , xent : float = 0.1 , reduction = \"mean\" ): \"\"\"Constructor for FocalCosineLoss. Todo: Add documentation. \"\"\" super ( FocalCosineLoss , self ) . __init__ () self . alpha = alpha self . gamma = gamma self . xent = xent self . reduction = reduction forward ( self , logits , target ) Forward Method. Source code in torchflare/criterion/focal_loss.py def forward ( self , logits : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method.\"\"\" cosine_loss = F . cosine_embedding_loss ( logits , torch . nn . functional . one_hot ( target , num_classes = logits . size ( - 1 )), torch . tensor ([ 1 ], device = target . device ), reduction = self . reduction , ) cent_loss = F . cross_entropy ( F . normalize ( logits ), target , reduction = \"none\" ) pt = torch . exp ( - cent_loss ) focal_loss = self . alpha * ( 1 - pt ) ** self . gamma * cent_loss if self . reduction == \"mean\" : focal_loss = torch . mean ( focal_loss ) return cosine_loss + self . xent * focal_loss FocalLoss Implementation of Focal Loss. Focal loss was proposed in Focal Loss for Dense Object Detection . Methods __init__ ( self , gamma = 0 , eps = 1e-07 , reduction = 'mean' ) special Constructor Method for FocalLoss class. Parameters: Name Type Description Default gamma The focal parameter. Defaults to 0. 0 eps Constant for computational stability. 1e-07 reduction The reduction parameter for Cross Entropy Loss. 'mean' Source code in torchflare/criterion/focal_loss.py def __init__ ( self , gamma = 0 , eps = 1e-7 , reduction = \"mean\" ): \"\"\"Constructor Method for FocalLoss class. Args: gamma : The focal parameter. Defaults to 0. eps : Constant for computational stability. reduction: The reduction parameter for Cross Entropy Loss. \"\"\" super ( FocalLoss , self ) . __init__ () self . gamma = gamma self . reduction = reduction self . eps = eps self . ce = torch . nn . CrossEntropyLoss ( reduction = \"none\" ) forward ( self , logits , targets ) Forward method. Parameters: Name Type Description Default logits Tensor The raw logits from the network of shape (N,C,*) where C = number of classes , * = extra dims required targets Tensor The targets of shape (N , *). required Returns: Type Description Tensor The computed loss value Source code in torchflare/criterion/focal_loss.py def forward ( self , logits : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: logits: The raw logits from the network of shape (N,C,*) where C = number of classes , * = extra dims targets: The targets of shape (N , *). Returns: The computed loss value \"\"\" logp = self . ce ( logits , targets ) p = torch . exp ( - logp ) loss = ( 1 - p ) ** self . gamma * logp return loss . mean () if self . reduction == \"mean\" else loss . sum () if self . reduction == \"sum\" else loss","title":"Focal Loss variants"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss-classes","text":"","title":"Classes"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.BCEFocalLoss","text":"Implementation of Focal Loss for Binary Classification Problems. Focal loss was proposed in Focal Loss for Dense Object Detection .","title":"BCEFocalLoss"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.BCEFocalLoss-methods","text":"","title":"Methods"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalCosineLoss","text":"Implementation Focal cosine loss. Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble . Source : https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/203271","title":"FocalCosineLoss"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalCosineLoss-methods","text":"","title":"Methods"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalLoss","text":"Implementation of Focal Loss. Focal loss was proposed in Focal Loss for Dense Object Detection .","title":"FocalLoss"},{"location":"criterion/focal_loss/#torchflare.criterion.focal_loss.FocalLoss-methods","text":"","title":"Methods"},{"location":"criterion/segmentation/","text":"Implements IouLoss. Classes IOULoss Computes intersection over union Loss. IOULoss = 1 - iou_score Methods __init__ ( self , class_dim = 1 ) special Constructor method for IOULoss. Parameters: Name Type Description Default class_dim indicates class dimension (K) for outputs and targets tensors (default = 1) 1 Source code in torchflare/criterion/iou_loss.py def __init__ ( self , class_dim = 1 ): \"\"\"Constructor method for IOULoss. Args: class_dim: indicates class dimension (K) for outputs and targets tensors (default = 1) \"\"\" super ( IOULoss , self ) . __init__ () self . iou = IOU ( threshold = None , class_dim = class_dim ) forward ( self , outputs , targets ) Forward Method. Parameters: Name Type Description Default outputs Tensor outputs from the net after applying activations. required targets Tensor The targets. required Returns: Type Description Tensor The computed loss value. Source code in torchflare/criterion/iou_loss.py def forward ( self , outputs : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method. Args: outputs: outputs from the net after applying activations. targets: The targets. Returns: The computed loss value. \"\"\" self . iou . reset () self . iou . accumulate ( outputs = outputs , targets = targets ) return 1 - self . iou . compute () Implements DiceLoss. Classes DiceLoss Implementation of Dice Loss. Methods __init__ ( self , class_dim = 1 ) special Constructor method for Dice Loss. Parameters: Name Type Description Default class_dim The dimension indication class. 1 Source code in torchflare/criterion/dice_loss.py def __init__ ( self , class_dim = 1 ): \"\"\"Constructor method for Dice Loss. Args: class_dim: The dimension indication class. \"\"\" super ( DiceLoss , self ) . __init__ () self . dice = DiceScore ( threshold = None , class_dim = class_dim ) forward ( self , outputs , targets ) Forward method. Parameters: Name Type Description Default outputs Tensor outputs from the net after applying activations. required targets Tensor The targets. required Returns: Type Description Tensor The computed loss value. Source code in torchflare/criterion/dice_loss.py def forward ( self , outputs : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward method. Args: outputs: outputs from the net after applying activations. targets: The targets. Returns: The computed loss value. \"\"\" self . dice . reset () self . dice . accumulate ( outputs = outputs , targets = targets ) return 1 - self . dice . compute ()","title":"Segmentation Losses"},{"location":"criterion/segmentation/#torchflare.criterion.iou_loss-classes","text":"","title":"Classes"},{"location":"criterion/segmentation/#torchflare.criterion.iou_loss.IOULoss","text":"Computes intersection over union Loss. IOULoss = 1 - iou_score","title":"IOULoss"},{"location":"criterion/segmentation/#torchflare.criterion.iou_loss.IOULoss-methods","text":"","title":"Methods"},{"location":"criterion/segmentation/#torchflare.criterion.dice_loss-classes","text":"","title":"Classes"},{"location":"criterion/segmentation/#torchflare.criterion.dice_loss.DiceLoss","text":"Implementation of Dice Loss.","title":"DiceLoss"},{"location":"criterion/segmentation/#torchflare.criterion.dice_loss.DiceLoss-methods","text":"","title":"Methods"},{"location":"criterion/triplet/","text":"Implements triplet loss. Classes TripletLoss Computes Triplet loss. Methods __init__ ( self , normalize_features = True , margin = None , hard_mining = True ) special Constructor method for TripletLoss. Parameters: Name Type Description Default normalize_features bool Whether to normalize the features. Default = True True margin float The value for margin. Default = None. None hard_mining bool Whether to use hard sample mining. Default = True. True Source code in torchflare/criterion/triplet_loss.py def __init__ ( self , normalize_features : bool = True , margin : float = None , hard_mining : bool = True , ): \"\"\"Constructor method for TripletLoss. Args: normalize_features: Whether to normalize the features. Default = True margin: The value for margin. Default = None. hard_mining: Whether to use hard sample mining. Default = True. \"\"\" super ( TripletLoss , self ) . __init__ () self . normalize_features = normalize_features self . margin = margin self . hard_mining = hard_mining forward ( self , embedding , targets ) Forward Method. Parameters: Name Type Description Default embedding Tensor The output of the network. required targets Tensor The targets. required Returns: Type Description Tensor The computed Triplet Loss. Source code in torchflare/criterion/triplet_loss.py def forward ( self , embedding : torch . Tensor , targets : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method. Args: embedding: The output of the network. targets: The targets. Returns: The computed Triplet Loss. \"\"\" distance_matrix = ( cosine_dist ( embedding , embedding ) if self . normalize_features else euclidean_dist ( embedding , embedding ) ) n = distance_matrix . size ( 0 ) pos_idxs = targets . view ( n , 1 ) . expand ( n , n ) . eq ( targets . view ( n , 1 ) . expand ( n , n ) . t ()) . float () neg_idxs = targets . view ( n , 1 ) . expand ( n , n ) . ne ( targets . view ( n , 1 ) . expand ( n , n ) . t ()) . float () if self . hard_mining : dist_ap , dist_an = hard_example_mining ( distance_matrix = distance_matrix , pos_idxs = pos_idxs , neg_idxs = neg_idxs ) else : dist_ap , dist_an = weighted_example_mining ( distance_matrix = distance_matrix , pos_idxs = pos_idxs , neg_idxs = neg_idxs ) y = dist_an . new () . resize_as_ ( dist_an ) . fill_ ( 1 ) if self . margin is not None and self . margin > 0 : loss = F . margin_ranking_loss ( dist_an , dist_ap , y , margin = self . margin ) else : loss = F . soft_margin_loss ( dist_an - dist_ap , y ) # fmt: off if loss == float ( \"Inf\" ): loss = F . margin_ranking_loss ( dist_an , dist_ap , y , margin = 0.3 ) # fmt: on return loss Functions hard_example_mining ( distance_matrix , pos_idxs , neg_idxs ) For each anchor, find the hardest positive and negative sample. Parameters: Name Type Description Default distance_matrix pair wise distance between samples, shape [N, M] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. Source code in torchflare/criterion/triplet_loss.py def hard_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the hardest positive and negative sample. Args: distance_matrix: pair wise distance between samples, shape [N, M] pos_idxs: positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note: Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 # `dist_ap` means distance(anchor, positive) # both `dist_ap` and `relative_p_inds` with shape [N] dist_ap , _ = torch . max ( distance_matrix * pos_idxs , dim = 1 ) # `dist_an` means distance(anchor, negative) # both `dist_an` and `relative_n_inds` with shape [N] dist_an , _ = torch . min ( distance_matrix * neg_idxs + pos_idxs * 99999999.0 , dim = 1 ) return dist_ap , dist_an weighted_example_mining ( distance_matrix , pos_idxs , neg_idxs ) For each anchor, find the weighted positive and negative sample. Parameters: Name Type Description Default distance_matrix pytorch Variable, pair wise distance between samples, shape [N, N] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] Source code in torchflare/criterion/triplet_loss.py def weighted_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the weighted positive and negative sample. Args: distance_matrix: pytorch Variable, pair wise distance between samples, shape [N, N] pos_idxs:positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 dist_ap = distance_matrix * pos_idxs dist_an = distance_matrix * neg_idxs weights_ap = softmax_weights ( dist_ap , pos_idxs ) weights_an = softmax_weights ( - dist_an , neg_idxs ) dist_ap = torch . sum ( dist_ap * weights_ap , dim = 1 ) dist_an = torch . sum ( dist_an * weights_an , dim = 1 ) return dist_ap , dist_an","title":"Triplet Loss"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss-classes","text":"","title":"Classes"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.TripletLoss","text":"Computes Triplet loss.","title":"TripletLoss"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.TripletLoss-methods","text":"","title":"Methods"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss-functions","text":"","title":"Functions"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.hard_example_mining","text":"For each anchor, find the hardest positive and negative sample. Parameters: Name Type Description Default distance_matrix pair wise distance between samples, shape [N, M] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. Source code in torchflare/criterion/triplet_loss.py def hard_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the hardest positive and negative sample. Args: distance_matrix: pair wise distance between samples, shape [N, M] pos_idxs: positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] p_inds: pytorch LongTensor, with shape [N]; indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1 n_inds: pytorch LongTensor, with shape [N]; indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1 Note: Only consider the case in which all targets have same num of samples, thus we can cope with all anchors in parallel. \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 # `dist_ap` means distance(anchor, positive) # both `dist_ap` and `relative_p_inds` with shape [N] dist_ap , _ = torch . max ( distance_matrix * pos_idxs , dim = 1 ) # `dist_an` means distance(anchor, negative) # both `dist_an` and `relative_n_inds` with shape [N] dist_an , _ = torch . min ( distance_matrix * neg_idxs + pos_idxs * 99999999.0 , dim = 1 ) return dist_ap , dist_an","title":"hard_example_mining()"},{"location":"criterion/triplet/#torchflare.criterion.triplet_loss.weighted_example_mining","text":"For each anchor, find the weighted positive and negative sample. Parameters: Name Type Description Default distance_matrix pytorch Variable, pair wise distance between samples, shape [N, N] required pos_idxs positive index with shape [N, M] required neg_idxs negative index with shape [N, M] required Returns: Type Description dist_ap pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] Source code in torchflare/criterion/triplet_loss.py def weighted_example_mining ( distance_matrix , pos_idxs , neg_idxs ): \"\"\"For each anchor, find the weighted positive and negative sample. Args: distance_matrix: pytorch Variable, pair wise distance between samples, shape [N, N] pos_idxs:positive index with shape [N, M] neg_idxs: negative index with shape [N, M] Returns: dist_ap: pytorch Variable, distance(anchor, positive); shape [N] dist_an: pytorch Variable, distance(anchor, negative); shape [N] \"\"\" assert len ( distance_matrix . size ()) == 2 # noqa: S101 dist_ap = distance_matrix * pos_idxs dist_an = distance_matrix * neg_idxs weights_ap = softmax_weights ( dist_ap , pos_idxs ) weights_an = softmax_weights ( - dist_an , neg_idxs ) dist_ap = torch . sum ( dist_ap * weights_ap , dim = 1 ) dist_an = torch . sum ( dist_an * weights_an , dim = 1 ) return dist_ap , dist_an","title":"weighted_example_mining()"},{"location":"datasets/image_data/","text":"Class to create the dataset for Image Classification. Methods from_df ( path , df , image_col , label_cols = None , augmentations = None , extension = None , convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset from the given dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None label_cols Union[str, List[str]] Column name or list of column names containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. Source code in torchflare/datasets/classification.py @classmethod def from_df ( cls , path : str , df : pd . DataFrame , image_col : str , label_cols : Union [ str , List [ str ]] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , extension : str = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from the given dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc augmentations: The batch_mixers to be used on images. label_cols: Column name or list of column names containing targets. extension : The image file extension. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. \"\"\" img_list = cls . _join_paths ( path = path , file_names = df . loc [:, image_col ] . values , extension = extension ) label_list = cls . _get_labels_from_df ( df = df , label_cols = label_cols ) return cls ( image_paths_list = img_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , ) from_folders ( path , augmentations = None , convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg Source code in torchflare/datasets/classification.py @classmethod def from_folders ( cls , path : str , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: path: The path where images are stored. augmentations:The batch_mixers to be used on images. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg \"\"\" class_to_idx = cls . _get_labels_from_folders ( path ) if class_to_idx : image_list , label_list = get_files_and_labels ( path , class_to_idx ) else : image_list , label_list = get_files ( path ), None return cls ( image_paths_list = image_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , ) Examples from_df from torchflare.datasets import ImageDataset ds = ImageDataset . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augmentations , extension = './jpg' convert_mode = \"RGB\" ) from_folders from torchflare.datasets import ImageDataset ds = ImageDataset . from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" )","title":"ImageDataset"},{"location":"datasets/image_data/#torchflare.datasets.classification.ImageDataset-methods","text":"","title":"Methods"},{"location":"datasets/image_data/#torchflare.datasets.classification.ImageDataset.from_df","text":"Classmethod to create pytorch dataset from the given dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None label_cols Union[str, List[str]] Column name or list of column names containing targets. None extension str The image file extension. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. Source code in torchflare/datasets/classification.py @classmethod def from_df ( cls , path : str , df : pd . DataFrame , image_col : str , label_cols : Union [ str , List [ str ]] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , extension : str = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from the given dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids along with image extension. i.e. the images should have names like img_215.jpg or img_name.png ,etc augmentations: The batch_mixers to be used on images. label_cols: Column name or list of column names containing targets. extension : The image file extension. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: For inference do not pass in the label_cols, keep it None. Augmentations : They must be Compose objects from albumentations or torchvision. When using albumentations do not use ToTensorV2(). extension : If you specify extension be it jpg,png,etc. Please include '.' in extension i.e. '.jpg' or '.png'. \"\"\" img_list = cls . _join_paths ( path = path , file_names = df . loc [:, image_col ] . values , extension = extension ) label_list = cls . _get_labels_from_df ( df = df , label_cols = label_cols ) return cls ( image_paths_list = img_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , )","title":"from_df()"},{"location":"datasets/image_data/#torchflare.datasets.classification.ImageDataset.from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default path str The path where images are stored. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images. None convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' Returns: Type Description ImageDataset return image_paths_list , labels_list , augmentations and convert_mode. Note Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg Source code in torchflare/datasets/classification.py @classmethod def from_folders ( cls , path : str , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , convert_mode : str = \"RGB\" , ) -> ImageDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: path: The path where images are stored. augmentations:The batch_mixers to be used on images. convert_mode: The mode to be passed to PIL.Image.convert. Returns: return image_paths_list , labels_list , augmentations and convert_mode. Note: Augmentations must be Compose objects from albumentations or torchvision. The training directory structure should be as follows: train/class_1/xxx.jpg . . train/class_n/xxz.jpg The test directory structure should be as follows: test_dir/xxx.jpg test_dir/xyz.jpg test_dir/ppp.jpg \"\"\" class_to_idx = cls . _get_labels_from_folders ( path ) if class_to_idx : image_list , label_list = get_files_and_labels ( path , class_to_idx ) else : image_list , label_list = get_files ( path ), None return cls ( image_paths_list = image_list , label_list = label_list , augmentations = augmentations , convert_mode = convert_mode , )","title":"from_folders()"},{"location":"datasets/image_data/#examples","text":"","title":"Examples"},{"location":"datasets/image_data/#from_df","text":"from torchflare.datasets import ImageDataset ds = ImageDataset . from_df ( df = train_df , path = \"/train/images\" , image_col = \"image_id\" , label_cols = \"label\" , augmentations = augmentations , extension = './jpg' convert_mode = \"RGB\" )","title":"from_df"},{"location":"datasets/image_data/#from_folders","text":"from torchflare.datasets import ImageDataset ds = ImageDataset . from_folders ( path = \"/train/images\" , augmentations = augs , convert_mode = \"RGB\" )","title":"from_folders"},{"location":"datasets/segmentation_data/","text":"Class to create a dataset for image segmentation tasks. Methods from_folders ( image_path , mask_path = None , augmentations = None , image_convert_mode = 'L' , mask_convert_mode = 'L' ) classmethod Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SegmentationDataset returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note If you want to create a dataset for testing just set mask_path = None. Source code in torchflare/datasets/segmentation.py @classmethod def from_folders ( cls , image_path : str , mask_path : str = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , image_convert_mode : str = \"L\" , mask_convert_mode : str = \"L\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: image_path: The path where images are stored. mask_path: The path where masks are stored. augmentations: The batch_mixers to apply on images and masks. image_convert_mode: The mode to be passed to PIL.Image.convert for input images mask_convert_mode: The mode to be passed to PIL.Image.convert for masks. Returns: returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note: If you want to create a dataset for testing just set mask_path = None. \"\"\" image_files = cls . _join_paths ( image_path , os . listdir ( image_path )) mask_files = cls . _join_paths ( mask_path , os . listdir ( mask_path )) if mask_path is not None else None return cls ( image_paths_list = image_files , mask_list = mask_files , augmentations = augmentations , image_convert_mode = image_convert_mode , mask_convert_mode = mask_convert_mode , ) from_rle ( path , df , image_col , mask_cols = None , augmentations = None , mask_size = None , num_classes = None , extension = None , image_convert_mode = 'RGB' ) classmethod Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images and the masks. None mask_cols List[str] The list of columns containing the rule length encoding. None mask_size Tuple[int, int] The size of mask. None num_classes int The number of num_classes. None image_convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' extension str The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc None Returns: Type Description SegmentationDataset returns image_paths_list , mask_list , image_convert_mode , augmentations and extra kwargs. Note If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. Source code in torchflare/datasets/segmentation.py @classmethod def from_rle ( cls , path : str , df : pd . DataFrame , image_col : str , mask_cols : List [ str ] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , mask_size : Tuple [ int , int ] = None , num_classes : int = None , extension : str = None , image_convert_mode : str = \"RGB\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids. augmentations: The batch_mixers to be used on images and the masks. mask_cols: The list of columns containing the rule length encoding. mask_size: The size of mask. num_classes: The number of num_classes. image_convert_mode: The mode to be passed to PIL.Image.convert. extension : The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc Returns: returns image_paths_list , mask_list , image_convert_mode , augmentations and extra kwargs. Note: If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. \"\"\" image_list = cls . _join_paths ( path = path , file_names = df [ image_col ] . values . tolist (), extension = extension ) mask_list = ( cls . create_mask_list ( df = df , image_col = image_col , mask_cols = mask_cols ) if mask_cols is not None else None ) return cls ( image_list , mask_list , augmentations , image_convert_mode = image_convert_mode , shape = mask_size , num_classes = num_classes , ) Examples from_df from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" , ) from_folders from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , )","title":"SegmentationDataset"},{"location":"datasets/segmentation_data/#torchflare.datasets.segmentation.SegmentationDataset-methods","text":"","title":"Methods"},{"location":"datasets/segmentation_data/#torchflare.datasets.segmentation.SegmentationDataset.from_folders","text":"Classmethod to create pytorch dataset from folders. Parameters: Name Type Description Default image_path str The path where images are stored. required mask_path str The path where masks are stored. None augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to apply on images and masks. None image_convert_mode str The mode to be passed to PIL.Image.convert for input images 'L' mask_convert_mode str The mode to be passed to PIL.Image.convert for masks. 'L' Returns: Type Description SegmentationDataset returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note If you want to create a dataset for testing just set mask_path = None. Source code in torchflare/datasets/segmentation.py @classmethod def from_folders ( cls , image_path : str , mask_path : str = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , image_convert_mode : str = \"L\" , mask_convert_mode : str = \"L\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset from folders. Args: image_path: The path where images are stored. mask_path: The path where masks are stored. augmentations: The batch_mixers to apply on images and masks. image_convert_mode: The mode to be passed to PIL.Image.convert for input images mask_convert_mode: The mode to be passed to PIL.Image.convert for masks. Returns: returns image_paths_list , mask_path , augmentations , image_convert_mode , mask_convert_mode. Note: If you want to create a dataset for testing just set mask_path = None. \"\"\" image_files = cls . _join_paths ( image_path , os . listdir ( image_path )) mask_files = cls . _join_paths ( mask_path , os . listdir ( mask_path )) if mask_path is not None else None return cls ( image_paths_list = image_files , mask_list = mask_files , augmentations = augmentations , image_convert_mode = image_convert_mode , mask_convert_mode = mask_convert_mode , )","title":"from_folders()"},{"location":"datasets/segmentation_data/#torchflare.datasets.segmentation.SegmentationDataset.from_rle","text":"Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Parameters: Name Type Description Default path str The path where images are saved. required df pd.DataFrame The dataframe containing the image name/ids, and the targets required image_col str The name of the image column containing the image name/ids. required augmentations Union[A.Compose, torchvision.transforms.Compose] The batch_mixers to be used on images and the masks. None mask_cols List[str] The list of columns containing the rule length encoding. None mask_size Tuple[int, int] The size of mask. None num_classes int The number of num_classes. None image_convert_mode str The mode to be passed to PIL.Image.convert. 'RGB' extension str The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc None Returns: Type Description SegmentationDataset returns image_paths_list , mask_list , image_convert_mode , augmentations and extra kwargs. Note If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. Source code in torchflare/datasets/segmentation.py @classmethod def from_rle ( cls , path : str , df : pd . DataFrame , image_col : str , mask_cols : List [ str ] = None , augmentations : Union [ A . Compose , torchvision . transforms . Compose ] = None , mask_size : Tuple [ int , int ] = None , num_classes : int = None , extension : str = None , image_convert_mode : str = \"RGB\" , ) -> SegmentationDataset : \"\"\"Classmethod to create pytorch dataset when you have rule length encodings for masks stored in a dataframe. Args: path: The path where images are saved. df: The dataframe containing the image name/ids, and the targets image_col: The name of the image column containing the image name/ids. augmentations: The batch_mixers to be used on images and the masks. mask_cols: The list of columns containing the rule length encoding. mask_size: The size of mask. num_classes: The number of num_classes. image_convert_mode: The mode to be passed to PIL.Image.convert. extension : The extension of image file. If your image_names do not have extension then set extension to '.jpg' or '.png' ,etc Returns: returns image_paths_list , mask_list , image_convert_mode , augmentations and extra kwargs. Note: If you want to create a dataset for testing set mask_cols = None, mask_size = None, num_classes = None. The created masks will be binary. \"\"\" image_list = cls . _join_paths ( path = path , file_names = df [ image_col ] . values . tolist (), extension = extension ) mask_list = ( cls . create_mask_list ( df = df , image_col = image_col , mask_cols = mask_cols ) if mask_cols is not None else None ) return cls ( image_list , mask_list , augmentations , image_convert_mode = image_convert_mode , shape = mask_size , num_classes = num_classes , )","title":"from_rle()"},{"location":"datasets/segmentation_data/#examples","text":"","title":"Examples"},{"location":"datasets/segmentation_data/#from_df","text":"from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_rle ( df = df , path = \"/train/images\" , image_col = \"image_id\" , mask_cols = [ \"EncodedPixles\" ], extension = \".jpg\" , mask_size = ( 320 , 320 ), num_classes = 4 , augmentations = augs , image_convert_mode = \"RGB\" , )","title":"from_df"},{"location":"datasets/segmentation_data/#from_folders","text":"from torchflare.datasets import SegmentationDataset ds = SegmentationDataset . from_folders ( image_path = \"/train/images\" , mask_path = \"/train/masks\" , augmentations = augs , image_convert_mode = \"L\" , mask_convert_mode = \"L\" , )","title":"from_folders"},{"location":"datasets/tabular_data/","text":"Class to create a dataset for tasks involving tabular data. Methods from_csv ( csv_path , feature_cols , label_cols = None ) classmethod Classmethod to create pytorch style dataset from csv file. Parameters: Name Type Description Default csv_path The full path to csv. required feature_cols Union[str, List[str]] The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of features , labels if label_cols is not None else return np.array of features. Source code in torchflare/datasets/tabular.py @classmethod def from_csv ( cls , csv_path , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from csv file. Args: csv_path: The full path to csv. feature_cols: The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of features , labels if label_cols is not None else return np.array of features. \"\"\" return cls . from_df ( df = pd . read_csv ( csv_path ), feature_cols = feature_cols , label_cols = label_cols ) from_df ( df , feature_cols , label_cols = None ) classmethod Classmethod to create pytorch style dataset from dataframes. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has features, and the labels/targets. required feature_cols Union[str, List[str]] The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of features , labels if label_cols is not None else return np.array of features. Source code in torchflare/datasets/tabular.py @classmethod def from_df ( cls , df : pd . DataFrame , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from dataframes. Args: df: The dataframe which has features, and the labels/targets. feature_cols: The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of features , labels if label_cols is not None else return np.array of features. \"\"\" features = df . loc [:, feature_cols ] . values labels = df . loc [:, label_cols ] . values if label_cols is not None else None return cls ( features = features , labels = labels ) Examples from_df from torchflare.datasets import TabularDataset ds = TabularDataset . from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" ) from_csv from torchflare.datasets import TabularDataset ds = TabularDataset . from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" )","title":"TabularDataset"},{"location":"datasets/tabular_data/#torchflare.datasets.tabular.TabularDataset-methods","text":"","title":"Methods"},{"location":"datasets/tabular_data/#torchflare.datasets.tabular.TabularDataset.from_csv","text":"Classmethod to create pytorch style dataset from csv file. Parameters: Name Type Description Default csv_path The full path to csv. required feature_cols Union[str, List[str]] The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of features , labels if label_cols is not None else return np.array of features. Source code in torchflare/datasets/tabular.py @classmethod def from_csv ( cls , csv_path , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from csv file. Args: csv_path: The full path to csv. feature_cols: The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of features , labels if label_cols is not None else return np.array of features. \"\"\" return cls . from_df ( df = pd . read_csv ( csv_path ), feature_cols = feature_cols , label_cols = label_cols )","title":"from_csv()"},{"location":"datasets/tabular_data/#torchflare.datasets.tabular.TabularDataset.from_df","text":"Classmethod to create pytorch style dataset from dataframes. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has features, and the labels/targets. required feature_cols Union[str, List[str]] The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. required label_cols Union[str, List[str]] The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. None Returns: Type Description TabularDataset np.array of features , labels if label_cols is not None else return np.array of features. Source code in torchflare/datasets/tabular.py @classmethod def from_df ( cls , df : pd . DataFrame , feature_cols : Union [ str , List [ str ]], label_cols : Union [ str , List [ str ]] = None , ) -> TabularDataset : \"\"\"Classmethod to create pytorch style dataset from dataframes. Args: df: The dataframe which has features, and the labels/targets. feature_cols: The name of columns which contain features. feature_cols can be a string if single column or can be a list of string if multiple columns. label_cols: The name of columns which contain the labels. label_cols can be a string or can be a list of string if multiple columns are used. Returns: np.array of features , labels if label_cols is not None else return np.array of features. \"\"\" features = df . loc [:, feature_cols ] . values labels = df . loc [:, label_cols ] . values if label_cols is not None else None return cls ( features = features , labels = labels )","title":"from_df()"},{"location":"datasets/tabular_data/#examples","text":"","title":"Examples"},{"location":"datasets/tabular_data/#from_df","text":"from torchflare.datasets import TabularDataset ds = TabularDataset . from_df ( df = df , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" )","title":"from_df"},{"location":"datasets/tabular_data/#from_csv","text":"from torchflare.datasets import TabularDataset ds = TabularDataset . from_csv ( csv_path = \"/train/train_data.csv\" , feature_cols = [ \"col1\" , \"col2\" ], label_cols = \"labels\" )","title":"from_csv"},{"location":"datasets/text_data/","text":"Class to create a dataset for text classification as required by transformers. Methods from_df ( df , input_col , tokenizer , max_len , label_cols = None ) classmethod Classmethod to create the dataset from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has the data. required input_col str The column containing the inputs. required label_cols Union[str, List[str]] The column which contains corresponding labels. None tokenizer The tokenizer to be used.(Use only tokenizer available in huggingface) required max_len int The max_len to be used. required Returns: Type Description TextClassificationDataset A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. Source code in torchflare/datasets/text_dataset.py @classmethod def from_df ( cls , df : pd . DataFrame , input_col : str , tokenizer , max_len : int , label_cols : Union [ str , List [ str ]] = None , ) -> TextClassificationDataset : \"\"\"Classmethod to create the dataset from dataframe. Args: df: The dataframe which has the data. input_col: The column containing the inputs. label_cols: The column which contains corresponding labels. tokenizer: The tokenizer to be used.(Use only tokenizer available in huggingface) max_len: The max_len to be used. Returns: A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. \"\"\" inputs = df . loc [:, input_col ] . values . tolist () labels = df . loc [:, label_cols ] . values . tolist () if label_cols is not None else None return cls ( inputs = inputs , labels = labels , tokenizer = tokenizer , max_len = max_len ) Examples from_df import transformers from torchflare.datasets import TextClassificationDataset tokenizer = transformers . BertTokenizer . from_pretrained ( \"bert-base-uncased\" ) ds = TextClassificationDataset . from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 )","title":"TextClassificationDataset"},{"location":"datasets/text_data/#torchflare.datasets.text_dataset.TextClassificationDataset-methods","text":"","title":"Methods"},{"location":"datasets/text_data/#torchflare.datasets.text_dataset.TextClassificationDataset.from_df","text":"Classmethod to create the dataset from dataframe. Parameters: Name Type Description Default df pd.DataFrame The dataframe which has the data. required input_col str The column containing the inputs. required label_cols Union[str, List[str]] The column which contains corresponding labels. None tokenizer The tokenizer to be used.(Use only tokenizer available in huggingface) required max_len int The max_len to be used. required Returns: Type Description TextClassificationDataset A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. Source code in torchflare/datasets/text_dataset.py @classmethod def from_df ( cls , df : pd . DataFrame , input_col : str , tokenizer , max_len : int , label_cols : Union [ str , List [ str ]] = None , ) -> TextClassificationDataset : \"\"\"Classmethod to create the dataset from dataframe. Args: df: The dataframe which has the data. input_col: The column containing the inputs. label_cols: The column which contains corresponding labels. tokenizer: The tokenizer to be used.(Use only tokenizer available in huggingface) max_len: The max_len to be used. Returns: A list of sentences and corresponding labels if label_cols is provided else return a list of sentences. \"\"\" inputs = df . loc [:, input_col ] . values . tolist () labels = df . loc [:, label_cols ] . values . tolist () if label_cols is not None else None return cls ( inputs = inputs , labels = labels , tokenizer = tokenizer , max_len = max_len )","title":"from_df()"},{"location":"datasets/text_data/#examples","text":"","title":"Examples"},{"location":"datasets/text_data/#from_df","text":"import transformers from torchflare.datasets import TextClassificationDataset tokenizer = transformers . BertTokenizer . from_pretrained ( \"bert-base-uncased\" ) ds = TextClassificationDataset . from_df ( df = df , input_col = \"tweet\" , label_cols = \"label\" , tokenizer = tokenizer , max_len = 128 )","title":"from_df"},{"location":"metrics/classification/accuracy/","text":"Computes Accuracy. Support binary,multilabel and multiclass cases Methods __init__ ( self , num_classes , threshold = 0.5 , multilabel = False ) special Constructor method for Accuracy Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/accuracy_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False ): \"\"\"Constructor method for Accuracy Class. Args: num_classes: The number of num_classes. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Accuracy , self ) . __init__ ( multilabel = multilabel ) self . threshold = threshold self . num_classes = num_classes self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/accuracy_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) compute ( self ) Computes the Accuracy per epoch. Returns: Type Description Tensor The accuracy Source code in torchflare/metrics/accuracy_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the Accuracy per epoch. Returns: The accuracy \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) correct , total = self . _compute ( outputs = outputs , targets = targets ) return correct / total handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/accuracy_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/accuracy_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import Accuracy # Binary-Classification Problems acc = Accuracy ( num_classes = 2 , threshold = 0.7 , multilabel = False ) # Mutliclass-Classification Problems multiclass_acc = Accuracy ( num_classes = 4 , multilabel = False ) # Multilabel-Classification Problems multilabel_acc = Accuracy ( num_classes = 5 , multilabel = True , threshold = 0.7 )","title":"Accuracy"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy-methods","text":"","title":"Methods"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.__init__","text":"Constructor method for Accuracy Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/accuracy_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False ): \"\"\"Constructor method for Accuracy Class. Args: num_classes: The number of num_classes. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Accuracy , self ) . __init__ ( multilabel = multilabel ) self . threshold = threshold self . num_classes = num_classes self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/accuracy_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.compute","text":"Computes the Accuracy per epoch. Returns: Type Description Tensor The accuracy Source code in torchflare/metrics/accuracy_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the Accuracy per epoch. Returns: The accuracy \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) correct , total = self . _compute ( outputs = outputs , targets = targets ) return correct / total","title":"compute()"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/accuracy_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/accuracy/#torchflare.metrics.accuracy_meter.Accuracy.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/accuracy_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/accuracy/#examples","text":"from torchflare.metrics import Accuracy # Binary-Classification Problems acc = Accuracy ( num_classes = 2 , threshold = 0.7 , multilabel = False ) # Mutliclass-Classification Problems multiclass_acc = Accuracy ( num_classes = 4 , multilabel = False ) # Multilabel-Classification Problems multilabel_acc = Accuracy ( num_classes = 5 , multilabel = True , threshold = 0.7 )","title":"Examples"},{"location":"metrics/classification/f1_score/","text":"Computes F1 Score. Supports binary,multiclass and multilabel cases. Methods __init__ ( self , num_classes , threshold = 0.5 , multilabel = False , average = 'macro' ) special Constructor method for F1-score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\". 'macro' multilabel bool Whether the problem is multilabel or not. False Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False , average : str = \"macro\" , ): \"\"\"Constructor method for F1-score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\". multilabel: Whether the problem is multilabel or not. \"\"\" super ( F1Score , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) compute ( self ) Method to compute FBeta Score. Returns: Type Description Tensor The computed F1-score Source code in torchflare/metrics/fbeta_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Method to compute FBeta Score. Returns: The computed F1-score \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) precision = tp / ( tp + fp + self . eps ) recall = tp / ( tp + fn + self . eps ) numerator = 2 * precision * recall denominator = precision + recall f1 = self . reduce ( numerator = numerator , denominator = denominator ) return f1 handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import F1Score # Binary-Classification Problems acc = F1Score ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = F1Score ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = F1Score ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"F1Score"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score-methods","text":"","title":"Methods"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.__init__","text":"Constructor method for F1-score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\". 'macro' multilabel bool Whether the problem is multilabel or not. False Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , num_classes : int , threshold : float = 0.5 , multilabel : bool = False , average : str = \"macro\" , ): \"\"\"Constructor method for F1-score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\". multilabel: Whether the problem is multilabel or not. \"\"\" super ( F1Score , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.compute","text":"Method to compute FBeta Score. Returns: Type Description Tensor The computed F1-score Source code in torchflare/metrics/fbeta_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Method to compute FBeta Score. Returns: The computed F1-score \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) precision = tp / ( tp + fp + self . eps ) recall = tp / ( tp + fn + self . eps ) numerator = 2 * precision * recall denominator = precision + recall f1 = self . reduce ( numerator = numerator , denominator = denominator ) return f1","title":"compute()"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/f1_score/#torchflare.metrics.fbeta_meter.F1Score.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/f1_score/#examples","text":"from torchflare.metrics import F1Score # Binary-Classification Problems acc = F1Score ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = F1Score ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = F1Score ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/classification/fbeta/","text":"Computes Fbeta Score. Supports binary,multiclass and multilabel cases. Methods __init__ ( self , beta , num_classes , threshold = 0.5 , average = 'macro' , multilabel = False ) special Constructor method for Fbeta score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\" 'macro' beta float weight of precision in harmonic mean. required multilabel bool Whether problem is multilabel or not. False Note In case of binary classification, set num_classes = 1 Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , beta : float , num_classes : int , threshold : float = 0.5 , average : str = \"macro\" , multilabel : bool = False , ): \"\"\"Constructor method for Fbeta score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\" beta : weight of precision in harmonic mean. multilabel: Whether problem is multilabel or not. Note: In case of binary classification, set num_classes = 1 \"\"\" super ( FBeta , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . beta = beta self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) compute ( self ) Computes the FBeta Score. Returns: Type Description Tensor The computed Fbeta score. Source code in torchflare/metrics/fbeta_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the FBeta Score. Returns: The computed Fbeta score. \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) precision = tp / ( tp + fp + self . eps ) recall = tp / ( tp + fn + self . eps ) numerator = ( 1 + self . beta ** 2 ) * precision * recall denominator = self . beta ** 2 * precision + recall fbeta = self . reduce ( numerator = numerator , denominator = denominator ) return fbeta handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import FBeta # Binary-Classification Problems acc = FBeta ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = FBeta ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = FBeta ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"FBeta"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta-methods","text":"","title":"Methods"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.__init__","text":"Constructor method for Fbeta score. Parameters: Name Type Description Default num_classes int The number of num_classes(For binary case , use out_features : 1) required threshold float The value of threshold for masking. Input is raw logits. 0.5 average str One of \"micro\" or \"macro\" 'macro' beta float weight of precision in harmonic mean. required multilabel bool Whether problem is multilabel or not. False Note In case of binary classification, set num_classes = 1 Source code in torchflare/metrics/fbeta_meter.py def __init__ ( self , beta : float , num_classes : int , threshold : float = 0.5 , average : str = \"macro\" , multilabel : bool = False , ): \"\"\"Constructor method for Fbeta score. Args: num_classes : The number of num_classes(For binary case , use out_features : 1) threshold: The value of threshold for masking. Input is raw logits. average : One of \"micro\" or \"macro\" beta : weight of precision in harmonic mean. multilabel: Whether problem is multilabel or not. Note: In case of binary classification, set num_classes = 1 \"\"\" super ( FBeta , self ) . __init__ ( num_classes = num_classes , multilabel = multilabel , threshold = threshold , average = average , ) self . beta = beta self . eps = 1e-20 self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor Ground truth targets required Source code in torchflare/metrics/fbeta_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : Ground truth targets \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.compute","text":"Computes the FBeta Score. Returns: Type Description Tensor The computed Fbeta score. Source code in torchflare/metrics/fbeta_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the FBeta Score. Returns: The computed Fbeta score. \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) precision = tp / ( tp + fp + self . eps ) recall = tp / ( tp + fn + self . eps ) numerator = ( 1 + self . beta ** 2 ) * precision * recall denominator = self . beta ** 2 * precision + recall fbeta = self . reduce ( numerator = numerator , denominator = denominator ) return fbeta","title":"compute()"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/fbeta_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/fbeta/#torchflare.metrics.fbeta_meter.FBeta.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/fbeta_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/fbeta/#examples","text":"from torchflare.metrics import FBeta # Binary-Classification Problems acc = FBeta ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = FBeta ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = FBeta ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/classification/precision/","text":"Class to compute Precision Score. Support binary, multiclass and multilabel cases Methods __init__ ( self , num_classes , average = 'macro' , threshold = 0.5 , multilabel = False ) special Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/precision_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Precision , self ) . __init__ ( threshold = threshold , num_classes = num_classes , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/precision_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) compute ( self ) Computes the Precision Score. Returns: Type Description Tensor The computed precision score. Source code in torchflare/metrics/precision_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the Precision Score. Returns: The computed precision score. \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) precision = self . reduce ( numerator = tp , denominator = tp + fp ) return precision handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/precision_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation Lists. Source code in torchflare/metrics/precision_meter.py def reset ( self ): \"\"\"Resets the accumulation Lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import Precision # Binary-Classification Problems acc = Precision ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Precision ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Precision ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Precision"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision-methods","text":"","title":"Methods"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.__init__","text":"Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/precision_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Precision , self ) . __init__ ( threshold = threshold , num_classes = num_classes , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/precision_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.compute","text":"Computes the Precision Score. Returns: Type Description Tensor The computed precision score. Source code in torchflare/metrics/precision_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the Precision Score. Returns: The computed precision score. \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) precision = self . reduce ( numerator = tp , denominator = tp + fp ) return precision","title":"compute()"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/precision_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/precision/#torchflare.metrics.precision_meter.Precision.reset","text":"Resets the accumulation Lists. Source code in torchflare/metrics/precision_meter.py def reset ( self ): \"\"\"Resets the accumulation Lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/precision/#examples","text":"from torchflare.metrics import Precision # Binary-Classification Problems acc = Precision ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Precision ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Precision ( num_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/classification/recall/","text":"Class to compute Recall Score. Support binary, multiclass and multilabel cases Methods __init__ ( self , num_classes , average = 'macro' , threshold = 0.5 , multilabel = False ) special Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/recall_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Recall , self ) . __init__ ( num_classes = num_classes , threshold = threshold , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/recall_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets ) compute ( self ) Compute the recall score. Returns: Type Description Tensor The computed recall score. Source code in torchflare/metrics/recall_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Compute the recall score. Returns: The computed recall score. \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) recall = self . reduce ( numerator = tp , denominator = tp + fn ) return recall handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/recall_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/recall_meter.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _outputs = [] self . _targets = [] Examples from torchflare.metrics import Recall # Binary-Classification Problems acc = Recall ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Recall ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Recallnum_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Recall"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall-methods","text":"","title":"Methods"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.__init__","text":"Constructor method for Precision Class. Parameters: Name Type Description Default num_classes int The number of num_classes. required average str The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. 'macro' threshold float The threshold value to transform probability predictions to binary values(0,1) 0.5 multilabel bool Set it to True if your problem is multilabel classification. False Source code in torchflare/metrics/recall_meter.py def __init__ ( self , num_classes : int , average : str = \"macro\" , threshold : float = 0.5 , multilabel : bool = False , ): \"\"\"Constructor method for Precision Class. Args: num_classes: The number of num_classes. average: The type of reduction to apply. macro: calculate metrics for each class and averages them with equal weightage to each class. micro: calculate metrics globally for each sample and class. threshold: The threshold value to transform probability predictions to binary values(0,1) multilabel: Set it to True if your problem is multilabel classification. \"\"\" super ( Recall , self ) . __init__ ( num_classes = num_classes , threshold = threshold , multilabel = multilabel , average = average , ) self . _outputs = None self . _targets = None self . reset ()","title":"__init__()"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs Tensor raw logits from the network. required targets Tensor targets to use for computing accuracy required Source code in torchflare/metrics/recall_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = self . detach_tensor ( outputs ), self . detach_tensor ( targets ) self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.compute","text":"Compute the recall score. Returns: Type Description Tensor The computed recall score. Source code in torchflare/metrics/recall_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Compute the recall score. Returns: The computed recall score. \"\"\" outputs = torch . cat ( self . _outputs ) targets = torch . cat ( self . _targets ) tp , fp , tn , fn = self . compute_stats ( outputs = outputs , targets = targets ) recall = self . reduce ( numerator = tp , denominator = tp + fn ) return recall","title":"compute()"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/recall_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/classification/recall/#torchflare.metrics.recall_meter.Recall.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/recall_meter.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/classification/recall/#examples","text":"from torchflare.metrics import Recall # Binary-Classification Problems acc = Recall ( num_classes = 2 , threshold = 0.7 , multilabel = False , average = \"macro\" ) # Mutliclass-Classification Problems multiclass_acc = Recall ( num_classes = 4 , multilabel = False , average = \"macro\" ) # Multilabel-Classification Problems multilabel_acc = Recallnum_classes = 5 , multilabel = True , threshold = 0.7 , average = \"macro\" )","title":"Examples"},{"location":"metrics/regression/mae/","text":"Computes Mean Absolute Error. Methods __init__ ( self ) special Constructor method for MAE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for MAE.\"\"\" self . _n_obs = None self . _abs_error_sum = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _abs_error_sum += torch . sum ( torch . abs ( outputs - targets )) self . _n_obs += targets . numel () compute ( self ) Computes the MAE. Returns: Type Description Tensor The computed MAE. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the MAE. Returns: The computed MAE. \"\"\" return self . _abs_error_sum / self . _n_obs handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _abs_error_sum = torch . tensor ( 0.0 )","title":"MAE"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE-methods","text":"","title":"Methods"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.__init__","text":"Constructor method for MAE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for MAE.\"\"\" self . _n_obs = None self . _abs_error_sum = None self . reset ()","title":"__init__()"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _abs_error_sum += torch . sum ( torch . abs ( outputs - targets )) self . _n_obs += targets . numel ()","title":"accumulate()"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.compute","text":"Computes the MAE. Returns: Type Description Tensor The computed MAE. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the MAE. Returns: The computed MAE. \"\"\" return self . _abs_error_sum / self . _n_obs","title":"compute()"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/mae/#torchflare.metrics.regression.MAE.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _abs_error_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/regression/mse/","text":"Computes Mean Squared Error. Methods __init__ ( self ) special Constructor Method for MSE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSE.\"\"\" self . _n_obs = None self . _squared_error_sum = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _squared_error_sum += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _n_obs += targets . numel () compute ( self ) Computes the MSE. Returns: Type Description Tensor The computed MSE. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the MSE. Returns: The computed MSE. \"\"\" return self . _squared_error_sum / self . _n_obs handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _squared_error_sum = torch . tensor ( 0.0 )","title":"MSE"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE-methods","text":"","title":"Methods"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.__init__","text":"Constructor Method for MSE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSE.\"\"\" self . _n_obs = None self . _squared_error_sum = None self . reset ()","title":"__init__()"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) self . _squared_error_sum += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _n_obs += targets . numel ()","title":"accumulate()"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.compute","text":"Computes the MSE. Returns: Type Description Tensor The computed MSE. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the MSE. Returns: The computed MSE. \"\"\" return self . _squared_error_sum / self . _n_obs","title":"compute()"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/mse/#torchflare.metrics.regression.MSE.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _squared_error_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/regression/msle/","text":"Computes Mean Squared Log Error. Methods __init__ ( self ) special Constructor Method for MSLE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSLE.\"\"\" self . _n_obs = None self . _log_squared_error_sum = None self . reset () accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) diff = torch . log1p ( outputs ) - torch . log1p ( targets ) self . _log_squared_error_sum += torch . sum ( torch . pow ( diff , 2 )) self . _n_obs += targets . numel () compute ( self ) Computes the MSLE. Returns: Type Description Tensor The computed MSLE. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the MSLE. Returns: The computed MSLE. \"\"\" return self . _log_squared_error_sum / self . _n_obs handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _log_squared_error_sum = torch . tensor ( 0.0 )","title":"MSLE"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE-methods","text":"","title":"Methods"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.__init__","text":"Constructor Method for MSLE. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor Method for MSLE.\"\"\" self . _n_obs = None self . _log_squared_error_sum = None self . reset ()","title":"__init__()"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" outputs , targets = detach_tensor ( outputs ), detach_tensor ( targets ) _check_shape ( outputs , targets ) diff = torch . log1p ( outputs ) - torch . log1p ( targets ) self . _log_squared_error_sum += torch . sum ( torch . pow ( diff , 2 )) self . _n_obs += targets . numel ()","title":"accumulate()"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.compute","text":"Computes the MSLE. Returns: Type Description Tensor The computed MSLE. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the MSLE. Returns: The computed MSLE. \"\"\" return self . _log_squared_error_sum / self . _n_obs","title":"compute()"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/msle/#torchflare.metrics.regression.MSLE.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ): \"\"\"Reset the output and target lists.\"\"\" self . _n_obs = torch . tensor ( 0 ) self . _log_squared_error_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/regression/r2/","text":"Computes R2-score. Methods __init__ ( self ) special Constructor method for R2-score. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for R2-score.\"\"\" self . _num_examples = None self . _sum_of_errors = None self . _y_sq_sum = None self . _y_sum = None accumulate ( self , outputs , targets ) Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" self . _num_examples += outputs . shape [ 0 ] self . _sum_of_errors += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _y_sum += torch . sum ( targets ) self . _y_sq_sum += torch . sum ( torch . pow ( targets , 2 )) compute ( self ) Computes the R2Score. Exceptions: Type Description ValueError If no examples are found. Returns: Type Description Tensor The computed R2Score. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the R2Score. Raises: ValueError: If no examples are found. Returns: The computed R2Score. \"\"\" if self . _num_examples == 0 : raise ValueError ( \"R2Score must have at least one example before it can be computed.\" ) return 1 - self . _sum_of_errors / ( self . _y_sq_sum - ( self . _y_sum ** 2 ) / self . _num_examples ) handle ( self ) Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ) -> None : \"\"\"Reset the output and target lists.\"\"\" self . _num_examples = 0 self . _sum_of_errors = torch . tensor ( 0.0 ) self . _y_sq_sum = torch . tensor ( 0.0 ) self . _y_sum = torch . tensor ( 0.0 )","title":"R2Score"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score-methods","text":"","title":"Methods"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.__init__","text":"Constructor method for R2-score. Source code in torchflare/metrics/regression.py def __init__ ( self ): \"\"\"Constructor method for R2-score.\"\"\" self . _num_examples = None self . _sum_of_errors = None self . _y_sq_sum = None self . _y_sum = None","title":"__init__()"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.accumulate","text":"Accumulates the batch outputs and targets. Parameters: Name Type Description Default outputs raw logits from the network. required targets targets to use for computing accuracy required Source code in torchflare/metrics/regression.py def accumulate ( self , outputs , targets ): \"\"\"Accumulates the batch outputs and targets. Args: outputs : raw logits from the network. targets : targets to use for computing accuracy \"\"\" self . _num_examples += outputs . shape [ 0 ] self . _sum_of_errors += torch . sum ( torch . pow ( outputs - targets , 2 )) self . _y_sum += torch . sum ( targets ) self . _y_sq_sum += torch . sum ( torch . pow ( targets , 2 ))","title":"accumulate()"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.compute","text":"Computes the R2Score. Exceptions: Type Description ValueError If no examples are found. Returns: Type Description Tensor The computed R2Score. Source code in torchflare/metrics/regression.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the R2Score. Raises: ValueError: If no examples are found. Returns: The computed R2Score. \"\"\" if self . _num_examples == 0 : raise ValueError ( \"R2Score must have at least one example before it can be computed.\" ) return 1 - self . _sum_of_errors / ( self . _y_sq_sum - ( self . _y_sum ** 2 ) / self . _num_examples )","title":"compute()"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.handle","text":"Method to get the class name. Returns: Type Description str The name of the class Source code in torchflare/metrics/regression.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The name of the class \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/regression/r2/#torchflare.metrics.regression.R2Score.reset","text":"Reset the output and target lists. Source code in torchflare/metrics/regression.py def reset ( self ) -> None : \"\"\"Reset the output and target lists.\"\"\" self . _num_examples = 0 self . _sum_of_errors = torch . tensor ( 0.0 ) self . _y_sq_sum = torch . tensor ( 0.0 ) self . _y_sum = torch . tensor ( 0.0 )","title":"reset()"},{"location":"metrics/segmentation/dice/","text":"Class to compute Dice Score. Methods __init__ ( self , threshold = None , class_dim = 1 ) special Constructor method for DiceScore. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/dice_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for DiceScore. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset () accumulate ( self , outputs , targets ) Class to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/dice_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Class to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets ) compute ( self ) Computes the dice score. Returns: Type Description Tensor The computed Dice score. Source code in torchflare/metrics/dice_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the dice score. Returns: The computed Dice score. \"\"\" self . _outputs = torch . cat ( self . _outputs ) self . _targets = torch . cat ( self . _targets ) tp , fp , fn = calculate_segmentation_statistics ( outputs = self . _outputs , targets = self . _targets , threshold = self . threshold , class_dim = self . class_dim , ) union = tp + fp + fn score = ( 2 * tp + self . eps * ( union == 0 ) . float ()) / ( 2 * tp + fp + fn + self . eps ) return torch . mean ( score ) handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/dice_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Resets the accumulation lists. Source code in torchflare/metrics/dice_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"DiceScore"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore-methods","text":"","title":"Methods"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.__init__","text":"Constructor method for DiceScore. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/dice_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for DiceScore. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset ()","title":"__init__()"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.accumulate","text":"Class to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/dice_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Class to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.compute","text":"Computes the dice score. Returns: Type Description Tensor The computed Dice score. Source code in torchflare/metrics/dice_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Computes the dice score. Returns: The computed Dice score. \"\"\" self . _outputs = torch . cat ( self . _outputs ) self . _targets = torch . cat ( self . _targets ) tp , fp , fn = calculate_segmentation_statistics ( outputs = self . _outputs , targets = self . _targets , threshold = self . threshold , class_dim = self . class_dim , ) union = tp + fp + fn score = ( 2 * tp + self . eps * ( union == 0 ) . float ()) / ( 2 * tp + fp + fn + self . eps ) return torch . mean ( score )","title":"compute()"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/dice_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/segmentation/dice/#torchflare.metrics.dice_meter.DiceScore.reset","text":"Resets the accumulation lists. Source code in torchflare/metrics/dice_meter.py def reset ( self ): \"\"\"Resets the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"metrics/segmentation/iou/","text":"Class which computes intersection over union. Methods __init__ ( self , threshold = None , class_dim = 1 ) special Constructor method for IOU. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/iou_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for IOU. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset () accumulate ( self , outputs , targets ) Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/iou_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets ) compute ( self ) Method to Compute IOU. Returns: Type Description Tensor The computed iou. Source code in torchflare/metrics/iou_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Method to Compute IOU. Returns: The computed iou. \"\"\" self . _outputs = torch . cat ( self . _outputs ) self . _targets = torch . cat ( self . _targets ) tp , fp , fn = calculate_segmentation_statistics ( outputs = self . _outputs , targets = self . _targets , threshold = self . threshold , class_dim = self . class_dim , ) union = tp + fp + fn score = ( tp + self . eps * ( union == 0 ) . float ()) / ( tp + fp + fn + self . eps ) return torch . mean ( score ) handle ( self ) Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/iou_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower () reset ( self ) Method to reset the accumulation lists. Source code in torchflare/metrics/iou_meter.py def reset ( self ): \"\"\"Method to reset the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"IOU"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU-methods","text":"","title":"Methods"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.__init__","text":"Constructor method for IOU. Parameters: Name Type Description Default threshold float threshold for binarization of predictions None class_dim int indicates class dimension (K) 1 Note Supports only binary cases Source code in torchflare/metrics/iou_meter.py def __init__ ( self , threshold : float = None , class_dim : int = 1 ): \"\"\"Constructor method for IOU. Args: threshold: threshold for binarization of predictions class_dim: indicates class dimension (K) Note: Supports only binary cases \"\"\" self . threshold = threshold self . class_dim = class_dim self . eps = 1e-20 self . _outputs = [] self . _targets = [] self . reset ()","title":"__init__()"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.accumulate","text":"Method to accumulate the outputs and targets. Parameters: Name Type Description Default outputs Tensor [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. required targets Tensor binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. required Source code in torchflare/metrics/iou_meter.py def accumulate ( self , outputs : torch . Tensor , targets : torch . Tensor ): \"\"\"Method to accumulate the outputs and targets. Args: outputs: [N, K, ...] tensor that for each of the N samples indicates the probability of the sample belonging to each of the K num_classes. targets: binary [N, K, ...] tensor that encodes which of the K num_classes are associated with the N-th sample. \"\"\" self . _outputs . append ( outputs ) self . _targets . append ( targets )","title":"accumulate()"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.compute","text":"Method to Compute IOU. Returns: Type Description Tensor The computed iou. Source code in torchflare/metrics/iou_meter.py def compute ( self ) -> torch . Tensor : \"\"\"Method to Compute IOU. Returns: The computed iou. \"\"\" self . _outputs = torch . cat ( self . _outputs ) self . _targets = torch . cat ( self . _targets ) tp , fp , fn = calculate_segmentation_statistics ( outputs = self . _outputs , targets = self . _targets , threshold = self . threshold , class_dim = self . class_dim , ) union = tp + fp + fn score = ( tp + self . eps * ( union == 0 ) . float ()) / ( tp + fp + fn + self . eps ) return torch . mean ( score )","title":"compute()"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.handle","text":"Method to get the class name. Returns: Type Description str The class name Source code in torchflare/metrics/iou_meter.py def handle ( self ) -> str : \"\"\"Method to get the class name. Returns: The class name \"\"\" return self . __class__ . __name__ . lower ()","title":"handle()"},{"location":"metrics/segmentation/iou/#torchflare.metrics.iou_meter.IOU.reset","text":"Method to reset the accumulation lists. Source code in torchflare/metrics/iou_meter.py def reset ( self ): \"\"\"Method to reset the accumulation lists.\"\"\" self . _outputs = [] self . _targets = []","title":"reset()"},{"location":"modules/airface/","text":"Implements LiArcFace. Classes LiArcFace Implementation of Li-ArcFace. AirFace: Lightweight and Efficient Model for Face Recognition Methods __init__ ( self , in_features , out_features , s = 64 , m = 0.45 ) special Constructor class of LiArcFace. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 64 m margin 0.45 Source code in torchflare/modules/airface.py def __init__ ( self , in_features , out_features , s = 64 , m = 0.45 ): \"\"\"Constructor class of LiArcFace. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( LiArcFace , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . s = s self . m = m self . eps = 1e-7 self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/airface.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" cos_theta = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return cos_theta cos_theta . clamp ( - 1 + self . eps , 1 - self . eps ) theta = torch . acos ( cos_theta ) one_hot = torch . zeros_like ( cos_theta ) one_hot . scatter_ ( 1 , targets . data . view ( - 1 , 1 ), 1 ) target = ( math . pi - 2 * ( theta + self . m )) / math . pi other = ( math . pi - 2 * theta ) / math . pi output = ( one_hot * target ) + (( 1.0 - one_hot ) * other ) output = output * self . s return output Examples import torch.nn as nn from torchflare.modules import LiArcFace layer = LiArcFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"Lightweight ArcFace"},{"location":"modules/airface/#torchflare.modules.airface-classes","text":"","title":"Classes"},{"location":"modules/airface/#torchflare.modules.airface.LiArcFace","text":"Implementation of Li-ArcFace. AirFace: Lightweight and Efficient Model for Face Recognition","title":"LiArcFace"},{"location":"modules/airface/#torchflare.modules.airface.LiArcFace-methods","text":"","title":"Methods"},{"location":"modules/airface/#examples","text":"import torch.nn as nn from torchflare.modules import LiArcFace layer = LiArcFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"Examples"},{"location":"modules/am_softmax/","text":"Implements AM-softmax. Classes AMSoftmax Implementation of Additive Margin Softmax for Face Verification. Additive Margin Softmax: https://arxiv.org/abs/1801.05599 Methods __init__ ( self , in_features , out_features , m = 0.35 , s = 32 ) special Class Constructor. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 32 m margin 0.35 Source code in torchflare/modules/am_softmax.py def __init__ ( self , in_features , out_features , m = 0.35 , s = 32 ): \"\"\"Class Constructor. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( AMSoftmax , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . m = m self . s = s self . eps = 1e-7 self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/am_softmax.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" cos_theta = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return cos_theta one_hot = torch . zeros_like ( cos_theta ) one_hot . scatter_ ( 1 , targets . view ( - 1 , 1 ) . long (), 1 ) logits = torch . where ( one_hot . bool (), cos_theta - self . m , cos_theta ) logits = torch . cos ( logits ) logits *= self . s return logits import torch.nn as nn from torchflare.modules import AMSoftmax layer = AMSoftmax ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"AMSoftmax"},{"location":"modules/am_softmax/#torchflare.modules.am_softmax-classes","text":"","title":"Classes"},{"location":"modules/am_softmax/#torchflare.modules.am_softmax.AMSoftmax","text":"Implementation of Additive Margin Softmax for Face Verification. Additive Margin Softmax: https://arxiv.org/abs/1801.05599","title":"AMSoftmax"},{"location":"modules/am_softmax/#torchflare.modules.am_softmax.AMSoftmax-methods","text":"","title":"Methods"},{"location":"modules/arcface/","text":"Implements ArcFace. Classes ArcFace Implementation of ArcFace. ArcFace: : Additive Angular Margin Loss for Deep Face Recognition Methods __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ) special Class Constructor. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 30.0 m margin 0.35 Source code in torchflare/modules/arcface.py def __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ): \"\"\"Class Constructor. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( ArcFace , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . s = s self . m = m self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) self . threshold = math . pi - self . m self . eps = 1e-7 forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/arcface.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" cos_theta = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return cos_theta theta = torch . acos ( torch . clamp ( cos_theta , - 1 + self . eps , 1 - self . eps )) one_hot = torch . zeros_like ( cos_theta ) one_hot . scatter_ ( 1 , targets . view ( - 1 , 1 ) . long (), 1 ) mask = torch . where ( theta > self . threshold , torch . zeros_like ( one_hot ), one_hot ) logits = torch . where ( mask . bool (), theta + self . m , theta ) logits = torch . cos ( logits ) logits *= self . s return logits import torch.nn as nn from torchflare.modules import ArcFace layer = ArcFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"ArcFace"},{"location":"modules/arcface/#torchflare.modules.arcface-classes","text":"","title":"Classes"},{"location":"modules/arcface/#torchflare.modules.arcface.ArcFace","text":"Implementation of ArcFace. ArcFace: : Additive Angular Margin Loss for Deep Face Recognition","title":"ArcFace"},{"location":"modules/arcface/#torchflare.modules.arcface.ArcFace-methods","text":"","title":"Methods"},{"location":"modules/cosface/","text":"Implements CosFace. Classes CosFace Implementation of CosFace. CosFace: Large Margin Cosine Loss for Deep Face Recognition Methods __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ) special Class Constructor. Parameters: Name Type Description Default in_features Size of the input features required out_features The size of output features(usually number of num_classes) required s The norm for input features. 30.0 m margin 0.35 Source code in torchflare/modules/cosface.py def __init__ ( self , in_features , out_features , s = 30.0 , m = 0.35 ): \"\"\"Class Constructor. Args: in_features: Size of the input features out_features: The size of output features(usually number of num_classes) s: The norm for input features. m: margin \"\"\" super ( CosFace , self ) . __init__ () self . in_features = in_features self . out_features = out_features self . s = s self . m = m self . Weight = nn . Parameter ( torch . FloatTensor ( self . out_features , self . in_features )) nn . init . xavier_uniform_ ( self . Weight ) forward ( self , features , targets = None ) Forward Pass. Parameters: Name Type Description Default features Tensor The input features of shape (BS x F) where BS is batch size and F is input feature dimension. required targets Tensor The targets with shape BS , where BS is batch size None Returns: Type Description Tensor Logits with shape (BS x out_features) Source code in torchflare/modules/cosface.py def forward ( self , features : torch . Tensor , targets : torch . Tensor = None ) -> torch . Tensor : \"\"\"Forward Pass. Args: features: The input features of shape (BS x F) where BS is batch size and F is input feature dimension. targets: The targets with shape BS , where BS is batch size Returns: Logits with shape (BS x out_features) \"\"\" # normalize features and weights logits = F . linear ( F . normalize ( features ), F . normalize ( self . Weight )) if targets is None : return logits # add margin target_logits = logits - self . m one_hot = torch . zeros_like ( logits ) one_hot . scatter_ ( 1 , targets . view ( - 1 , 1 ) . long (), 1 ) output = logits * ( 1 - one_hot ) + target_logits * one_hot # feature re-scale output *= self . s return output import torch.nn as nn from torchflare.modules import CosFace layer = CosFace ( in_features = 1024 , out_features = 256 , m = 0.45 , s = 64 ) crit = nn . CrossEntropyLoss () logits = layer ( emebedding , targets ) loss = crit ( logits , targets )","title":"CosFace"},{"location":"modules/cosface/#torchflare.modules.cosface-classes","text":"","title":"Classes"},{"location":"modules/cosface/#torchflare.modules.cosface.CosFace","text":"Implementation of CosFace. CosFace: Large Margin Cosine Loss for Deep Face Recognition","title":"CosFace"},{"location":"modules/cosface/#torchflare.modules.cosface.CosFace-methods","text":"","title":"Methods"},{"location":"modules/se_modules/","text":"Implementation of Squeeze and Excitation BLocks. Classes CSE Implementation of Channel Wise Squeeze and Excitation Block. Paper : https://arxiv.org/abs/1709.01507 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939 and https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178 Methods __init__ ( self , in_channels , r = 16 ) special Constructor for CSE class. Parameters: Name Type Description Default in_channels int The number of input channels in the feature map. required r int The reduction ration (Default : 16) 16 Source code in torchflare/modules/se_modules.py def __init__ ( self , in_channels : int , r : int = 16 ): \"\"\"Constructor for CSE class. Args: in_channels : The number of input channels in the feature map. r : The reduction ration (Default : 16) \"\"\" super ( CSE , self ) . __init__ () self . in_channels = in_channels self . r = r self . linear1 = nn . Linear ( self . in_channels , self . in_channels // self . r ) self . linear2 = nn . Linear ( self . in_channels // r , self . in_channels ) forward ( self , x ) Forward Method. Parameters: Name Type Description Default x Tensor The input tensor of shape (batch, channels, height, width) required Returns: Type Description Tensor Tensor of same shape Source code in torchflare/modules/se_modules.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\"Forward Method. Args: x: The input tensor of shape (batch, channels, height, width) Returns: Tensor of same shape \"\"\" x_inp = x x = x . view ( * ( x . shape [: - 2 ]), - 1 ) . mean ( - 1 ) x = F . relu ( self . linear1 ( x ), inplace = True ) x = self . linear2 ( x ) x = x . unsqueeze ( - 1 ) . unsqueeze ( - 1 ) x = torch . sigmoid ( x ) x = torch . mul ( x_inp , x ) return x SCSE Implementation of SCSE : Concurrent Spatial and Channel Squeeze and Channel Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178 Methods __init__ ( self , in_channels , r = 16 ) special Constructor for SCSE class. Parameters: Name Type Description Default in_channels The number of input channels in the feature map. required r The reduction ration (Default : 16) 16 Source code in torchflare/modules/se_modules.py def __init__ ( self , in_channels , r = 16 ): \"\"\"Constructor for SCSE class. Args: in_channels : The number of input channels in the feature map. r : The reduction ration (Default : 16) \"\"\" super ( SCSE , self ) . __init__ () self . in_channels = in_channels self . r = r self . cse = CSE ( in_channels = self . in_channels , r = self . r ) self . sse = SSE ( in_channels = self . in_channels ) forward ( self , x ) Forward method. Parameters: Name Type Description Default x The input tensor of shape (batch, channels, height, width) required Returns: Type Description Tensor Tensor of same shape Source code in torchflare/modules/se_modules.py def forward ( self , x ) -> torch . Tensor : \"\"\"Forward method. Args: x: The input tensor of shape (batch, channels, height, width) Returns: Tensor of same shape \"\"\" cse = self . cse ( x ) sse = self . sse ( x ) op = torch . add ( cse , sse ) return op SSE SSE : Channel Squeeze and Spatial Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178 Methods __init__ ( self , in_channels ) special Constructor method for SSE class. Parameters: Name Type Description Default in_channels The number of input channels in the feature map. required Source code in torchflare/modules/se_modules.py def __init__ ( self , in_channels ): \"\"\"Constructor method for SSE class. Args: in_channels : The number of input channels in the feature map. \"\"\" super ( SSE , self ) . __init__ () self . in_channels = in_channels # noinspection PyTypeChecker self . conv = nn . Conv2d ( in_channels = self . in_channels , out_channels = 1 , kernel_size = 1 , stride = 1 ) forward ( self , x ) Forward Method. Parameters: Name Type Description Default x The input tensor of shape (batch, channels, height, width) required Returns: Type Description Tensor Tensor of same shape Source code in torchflare/modules/se_modules.py def forward ( self , x ) -> torch . Tensor : \"\"\"Forward Method. Args: x: The input tensor of shape (batch, channels, height, width) Returns: Tensor of same shape \"\"\" x_inp = x x = self . conv ( x ) x = torch . sigmoid ( x ) x = torch . mul ( x_inp , x ) return x","title":"Sqeeze and Excitation Blocks"},{"location":"modules/se_modules/#torchflare.modules.se_modules-classes","text":"","title":"Classes"},{"location":"modules/se_modules/#torchflare.modules.se_modules.CSE","text":"Implementation of Channel Wise Squeeze and Excitation Block. Paper : https://arxiv.org/abs/1709.01507 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939 and https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178","title":"CSE"},{"location":"modules/se_modules/#torchflare.modules.se_modules.CSE-methods","text":"","title":"Methods"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SCSE","text":"Implementation of SCSE : Concurrent Spatial and Channel Squeeze and Channel Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178","title":"SCSE"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SCSE-methods","text":"","title":"Methods"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SSE","text":"SSE : Channel Squeeze and Spatial Excitation block. Paper : https://arxiv.org/abs/1803.02579 Adapted from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178","title":"SSE"},{"location":"modules/se_modules/#torchflare.modules.se_modules.SSE-methods","text":"","title":"Methods"}]}